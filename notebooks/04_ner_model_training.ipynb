{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: NER Model Training\n",
    "\n",
    "Train a BERT-based Named Entity Recognition model on the annotated allergen samples. This notebook focuses on model training only. Evaluation is handled separately in Notebook 05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: NER Model Training\n",
    "\n",
    "This notebook trains a Named Entity Recognition (NER) model to detect allergen mentions in OCR-extracted ingredient text. Compared to rule-based keyword matching, NER can better handle synonyms, context, multi-word entities, and negations.\n",
    "\n",
    "## Objectives\n",
    "- Load and prepare annotated data (train.json, val.json, test.json)\n",
    "- Initialize and fine-tune a BERT-based token classification model\n",
    "- Train with optimized hyperparameters and evaluate with seqeval metrics\n",
    "- Save the trained model for integration into the detection pipeline\n",
    "\n",
    "## Expected Improvements\n",
    "- Rule-based baseline: ~39% accuracy (current)\n",
    "- NER model target: 50–55% accuracy (+10–15% improvement)\n",
    "- More robust handling of variations and context\n",
    "\n",
    "Proceed to the next cells to configure the environment, load data, and train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A: Setup and Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "PyTorch version: 2.9.1+cpu\n",
      "CUDA available: False\n",
      "Training data directory: d:\\APU Materials\\Year 3 Semester 2\\FYP\\allergen-detection-fyp\\data\\ner_training\n",
      "Model output directory: d:\\APU Materials\\Year 3 Semester 2\\FYP\\allergen-detection-fyp\\models\\ner_model\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers torch datasets seqeval scikit-learn pandas numpy\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Project paths\n",
    "ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATA = ROOT / 'data'\n",
    "NER_TRAINING = DATA / 'ner_training'\n",
    "MODEL_DIR = ROOT / 'models' / 'ner_model'\n",
    "\n",
    "sys.path.insert(0, str(ROOT / 'src'))\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Training data directory: {NER_TRAINING}\")\n",
    "print(f\"Model output directory: {MODEL_DIR}\")\n",
    "\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Training Data and Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allergen labels (12): ['GLUTEN', 'MILK', 'EGG', 'PEANUT', 'TREE_NUT', 'SOY', 'FISH', 'SHELLFISH', 'SESAME', 'MUSTARD', 'CELERY', 'SULFITES']\n",
      "\n",
      "✓ Loaded annotated datasets:\n",
      "  Train: 9 samples\n",
      "  Val: 2 samples\n",
      "  Test: 3 samples\n"
     ]
    }
   ],
   "source": [
    "# Load label mapping\n",
    "with open(NER_TRAINING / 'label_mapping.json', 'r') as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "labels = label_mapping['labels']\n",
    "label2id = label_mapping['label_to_id']\n",
    "id2label = {int(k): v for k, v in label_mapping['id_to_label'].items()}\n",
    "\n",
    "print(f\"Allergen labels ({len(labels)}): {labels}\")\n",
    "\n",
    "# Check for split datasets\n",
    "train_file = NER_TRAINING / 'train.json'\n",
    "val_file = NER_TRAINING / 'val.json'\n",
    "test_file = NER_TRAINING / 'test.json'\n",
    "\n",
    "if not train_file.exists():\n",
    "    print(\"\\n⚠️  Training data not yet split!\")\n",
    "    print(f\"Please complete annotation in: {NER_TRAINING / 'annotation_template.json'}\")\n",
    "    print(\"Then split into train/val/test (70/15/15) and save as train.json, val.json, test.json\")\n",
    "    print(\"\\nProceeding with demo on annotation_template for now...\")\n",
    "    \n",
    "    # Load template for demonstration\n",
    "    with open(NER_TRAINING / 'annotation_template.json', 'r') as f:\n",
    "        template_data = json.load(f)\n",
    "    \n",
    "    print(f\"\\nLoaded {len(template_data)} samples from annotation template\")\n",
    "    print(f\"Sample: {template_data[0]['text'][:100]}...\")\n",
    "    \n",
    "    # For demo purposes, create a minimal split\n",
    "    n = len(template_data)\n",
    "    train_split = int(0.7 * n)\n",
    "    val_split = int(0.85 * n)\n",
    "    \n",
    "    train_data = template_data[:train_split]\n",
    "    val_data = template_data[train_split:val_split]\n",
    "    test_data = template_data[val_split:]\n",
    "    \n",
    "    print(f\"\\nDemo split: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test\")\n",
    "else:\n",
    "    with open(train_file, 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "    with open(val_file, 'r') as f:\n",
    "        val_data = json.load(f)\n",
    "    with open(test_file, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    print(f\"\\n✓ Loaded annotated datasets:\")\n",
    "    print(f\"  Train: {len(train_data)} samples\")\n",
    "    print(f\"  Val: {len(val_data)} samples\")\n",
    "    print(f\"  Test: {len(test_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tokenization and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9/9 [00:00<00:00, 260.65 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 129.74 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 134.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Datasets tokenized and ready for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, Features, Sequence, Value\n",
    "\n",
    "# Initialize tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def normalize_entities(entity_lists):\n",
    "    \"\"\"Ensure entities are structured dicts with start/end/label.\"\"\"\n",
    "    normalized = []\n",
    "    for ent_list in entity_lists:\n",
    "        norm = []\n",
    "        for ent in ent_list:\n",
    "            if isinstance(ent, list):\n",
    "                start, end, label = ent[0], ent[1], ent[2]\n",
    "            else:\n",
    "                start, end, label = ent.get(\"start\"), ent.get(\"end\"), ent.get(\"label\")\n",
    "            norm.append({\"start\": int(start), \"end\": int(end), \"label\": str(label)})\n",
    "        normalized.append(norm)\n",
    "    return normalized\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"Tokenize text and align entity labels to subword tokens.\"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        is_split_into_words=False,\n",
    "    )\n",
    "    \n",
    "    labels_local = []\n",
    "    for i, (text, entities) in enumerate(zip(examples[\"text\"], examples[\"entities\"])):\n",
    "        char_labels = ['O'] * len(text)\n",
    "        for ent in entities:\n",
    "            start, end, label = ent[\"start\"], ent[\"end\"], ent[\"label\"]\n",
    "            if start < len(text) and end <= len(text):\n",
    "                for j in range(start, end):\n",
    "                    char_labels[j] = label\n",
    "        \n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                offsets = tokenized_inputs.encodings[i].offsets[len(label_ids)]\n",
    "                if offsets[0] < len(char_labels):\n",
    "                    label = char_labels[offsets[0]]\n",
    "                    if label == 'O':\n",
    "                        label_ids.append(-100)\n",
    "                    else:\n",
    "                        label_ids.append(label2id[label])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "        labels_local.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels_local\n",
    "    return tokenized_inputs\n",
    "\n",
    "features = Features({\n",
    "    \"text\": Value(\"string\"),\n",
    "    \"entities\": Sequence({\n",
    "        \"start\": Value(\"int64\"),\n",
    "        \"end\": Value(\"int64\"),\n",
    "        \"label\": Value(\"string\")\n",
    "    })\n",
    "})\n",
    "\n",
    "# Convert to HF datasets with normalized entities\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"text\": [d[\"text\"] for d in train_data],\n",
    "    \"entities\": normalize_entities([d.get(\"entities\", []) for d in train_data])\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    \"text\": [d[\"text\"] for d in val_data],\n",
    "    \"entities\": normalize_entities([d.get(\"entities\", []) for d in val_data])\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"text\": [d[\"text\"] for d in test_data],\n",
    "    \"entities\": normalize_entities([d.get(\"entities\", []) for d in test_data])\n",
    "})\n",
    "\n",
    "# Tokenize\n",
    "train_tokenized = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "val_tokenized = val_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_tokenized = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "print(f\"✓ Datasets tokenized and ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity checks only — formal evaluation is in Notebook 05.\n",
      "Timestamp: 2025-12-20T15:28:25\n",
      "Model not yet defined in this session: name 'model' is not defined\n",
      "Example input_ids length: 512\n",
      "Example labels length: 512\n",
      "Forward pass skipped (no model or tokenized data present).\n"
     ]
    }
   ],
   "source": [
    "# Quick Sanity Checks (no metrics)\n",
    "print(\"Sanity checks only — formal evaluation is in Notebook 05.\")\n",
    "\n",
    "# 1) Show model basics if available\n",
    "try:\n",
    "    from datetime import datetime\n",
    "    print(\"Timestamp:\", datetime.now().isoformat(timespec='seconds'))\n",
    "    print(\"Model name:\", getattr(model.config, 'name_or_path', 'unknown'))\n",
    "    print(\"Num labels:\", getattr(model.config, 'num_labels', 'unknown'))\n",
    "except Exception as e:\n",
    "    print(\"Model not yet defined in this session:\", e)\n",
    "\n",
    "# 2) Peek at one training example if dataset objects exist\n",
    "try:\n",
    "    example = train_tokenized[0] if 'train_tokenized' in globals() else None\n",
    "    if example:\n",
    "        print(\"Example input_ids length:\", len(example['input_ids']))\n",
    "        print(\"Example labels length:\", len(example.get('labels', [])))\n",
    "    else:\n",
    "        print(\"No tokenized training data in current kernel session.\")\n",
    "except Exception as e:\n",
    "    print(\"Could not peek dataset:\", e)\n",
    "\n",
    "# 3) Optional: make a single forward pass to verify shape\n",
    "try:\n",
    "    import torch\n",
    "    if 'model' in globals() and 'train_tokenized' in globals() and len(train_tokenized) > 0:\n",
    "        batch = train_tokenized[:2]\n",
    "        inputs = {\n",
    "            'input_ids': torch.tensor(batch['input_ids']),\n",
    "            'attention_mask': torch.tensor(batch['attention_mask'])\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            out = model(**inputs)\n",
    "        print(\"Forward pass OK. Logits shape:\", tuple(out.logits.shape))\n",
    "    else:\n",
    "        print(\"Forward pass skipped (no model or tokenized data present).\")\n",
    "except Exception as e:\n",
    "    print(\"Forward pass failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING NER MODEL (manual loop)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\APU Materials\\Year 3 Semester 2\\FYP\\allergen-detection-fyp\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: TREE_NUT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "d:\\APU Materials\\Year 3 Semester 2\\FYP\\allergen-detection-fyp\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: GLUTEN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=2.2549 val_loss=1.6467 f1=0.286 prec=0.250 rec=0.333\n",
      "Epoch 2: train_loss=1.4494 val_loss=1.2942 f1=0.000 prec=0.000 rec=0.000\n",
      "Epoch 3: train_loss=1.7536 val_loss=1.1839 f1=0.000 prec=0.000 rec=0.000\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\APU Materials\\Year 3 Semester 2\\FYP\\allergen-detection-fyp\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PEANUT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': np.float64(1.8011715412139893), 'precision': np.float64(0.2), 'recall': np.float64(0.3333333333333333), 'f1': np.float64(0.25), 'accuracy': 0.4}\n",
      "\n",
      "✓ Trained NER model saved to: d:\\APU Materials\\Year 3 Semester 2\\FYP\\allergen-detection-fyp\\models\\ner_model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForTokenClassification, DataCollatorForTokenClassification\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model and collator\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True, max_length=512)\n",
    "\n",
    "# Strip non-tensor columns and set torch format\n",
    "keep_cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_ds = train_tokenized.remove_columns([c for c in train_tokenized.column_names if c not in keep_cols]).with_format(\"torch\")\n",
    "val_ds = val_tokenized.remove_columns([c for c in val_tokenized.column_names if c not in keep_cols]).with_format(\"torch\")\n",
    "test_ds = test_tokenized.remove_columns([c for c in test_tokenized.column_names if c not in keep_cols]).with_format(\"torch\")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=data_collator)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels_batch = batch.pop(\"labels\")\n",
    "            outputs = model(**batch, labels=labels_batch)\n",
    "            losses.append(outputs.loss.item())\n",
    "            logits = outputs.logits.detach().cpu().numpy()\n",
    "            labels_np = labels_batch.cpu().numpy()\n",
    "            for logit_seq, label_seq in zip(logits, labels_np):\n",
    "                pred_seq = logit_seq.argmax(-1)\n",
    "                seq_preds, seq_labels = [], []\n",
    "                for p, l in zip(pred_seq, label_seq):\n",
    "                    if l == -100:\n",
    "                        continue\n",
    "                    seq_preds.append(id2label[int(p)])\n",
    "                    seq_labels.append(id2label[int(l)])\n",
    "                if seq_labels:\n",
    "                    all_preds.append(seq_preds)\n",
    "                    all_labels.append(seq_labels)\n",
    "    if not all_labels:\n",
    "        return {\"loss\": np.mean(losses) if losses else None, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"accuracy\": 0.0}\n",
    "    return {\n",
    "        \"loss\": np.mean(losses) if losses else None,\n",
    "        \"precision\": precision_score(all_labels, all_preds),\n",
    "        \"recall\": recall_score(all_labels, all_preds),\n",
    "        \"f1\": f1_score(all_labels, all_preds),\n",
    "        \"accuracy\": accuracy_score(all_labels, all_preds),\n",
    "    }\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING NER MODEL (manual loop)\")\n",
    "print(\"=\"*60)\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        labels_batch = batch.pop(\"labels\")\n",
    "        outputs = model(**batch, labels=labels_batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / max(1, len(train_loader))\n",
    "    val_metrics = evaluate(val_loader)\n",
    "    print(f\"Epoch {epoch}: train_loss={avg_train_loss:.4f} val_loss={val_metrics['loss']:.4f} f1={val_metrics['f1']:.3f} prec={val_metrics['precision']:.3f} rec={val_metrics['recall']:.3f}\")\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_metrics = evaluate(test_loader)\n",
    "print(test_metrics)\n",
    "\n",
    "# Save trained model and label mapping\n",
    "model.save_pretrained(MODEL_DIR)\n",
    "tokenizer.save_pretrained(MODEL_DIR)\n",
    "with open(MODEL_DIR / 'label_mapping.json', 'w') as f:\n",
    "    json.dump({\n",
    "        \"labels\": labels,\n",
    "        \"label_to_id\": label2id,\n",
    "        \"id_to_label\": id2label\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Trained NER model saved to: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Save Trained Model and Checkpoint\n",
    "\n",
    "Note: Formal evaluation of the trained model is performed in Notebook 05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m test_results = \u001b[43mtrainer\u001b[49m.evaluate(test_tokenized)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTEST SET RESULTS\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(test_tokenized)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Precision: {test_results['eval_precision']:.3f}\")\n",
    "print(f\"  Recall:    {test_results['eval_recall']:.3f}\")\n",
    "print(f\"  F1 Score:  {test_results['eval_f1']:.3f}\")\n",
    "print(f\"  Accuracy:  {test_results['eval_accuracy']:.3f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(MODEL_DIR)\n",
    "tokenizer.save_pretrained(MODEL_DIR)\n",
    "\n",
    "# Save label mapping\n",
    "with open(MODEL_DIR / 'label_mapping.json', 'w') as f:\n",
    "    json.dump(label_mapping, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Model saved to: {MODEL_DIR}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Integrate NER model into allergen detection pipeline\")\n",
    "print(\"  2. Run comprehensive evaluation (Notebook 05)\")\n",
    "print(\"  3. Compare NER vs rule-based accuracy\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOAOO6C8D31RJ7Xf0iUk5Tc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
