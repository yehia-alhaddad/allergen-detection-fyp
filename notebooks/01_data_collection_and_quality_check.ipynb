{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section A: Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- **Environment:** Google Colab (recommended for data persistence via Drive)\n",
    "- **Credentials:** Google account with Drive access\n",
    "- **API:** Open Food Facts (free, no key required)\n",
    "\n",
    "Run cells in order from top to bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01: Data Collection and Quality Check\n",
    "\n",
    "Collect product images and allergen data from Open Food Facts API, with focus on Malaysia snacks and popular English-language brands.\n",
    "\n",
    "**Outputs:**\n",
    "- Raw images: `data/raw/` (product images and ingredient labels)\n",
    "- Dataset metadata: `data/annotations.csv` and `data/annotations_malaysia_only.csv`\n",
    "- Dataset inventory: `allergen_dictionary.json`\n",
    "\n",
    "**Environment:** Google Colab (mounts Google Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4XUtp3rplWx"
   },
   "outputs": [],
   "source": [
    "# Step 1: Install required packages\n",
    "!pip install -q requests pandas tqdm pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUyZWMU2plZt"
   },
   "outputs": [],
   "source": [
    "# Step 2: Import libraries and setup Google Colab\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import time\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive for data persistence\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrwjgF9TplgQ"
   },
   "outputs": [],
   "source": [
    "# Step 3: Setup data directory structure\n",
    "\n",
    "# Define base directory in Google Drive for saving data\n",
    "data_dir = \"/content/drive/MyDrive/allergen-detection-fyp/data\"\n",
    "os.makedirs(os.path.join(data_dir, \"raw\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, \"processed\"), exist_ok=True)\n",
    "\n",
    "print(\"‚úì Setup complete\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Raw images folder: {os.path.join(data_dir, 'raw')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VadYTRJSsIVC"
   },
   "source": [
    "# Step 4: Configure Open Food Facts API and Search Strategy\n",
    "\n",
    "Configure the Open Food Facts API endpoint and define search parameters for fetching Malaysia snacks and popular English-language brands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API endpoint and query fields\n",
    "base_url = \"https://world.openfoodfacts.org/api/v2/search\"\n",
    "\n",
    "# Fields to retrieve (focus on English-language data)\n",
    "fields = [\n",
    "    \"code\", \"product_name\", \"lang\",\n",
    "    \"selected_images\",\n",
    "    \"image_ingredients_url\",\n",
    "    \"image_url\",\n",
    "    \"allergens_tags\", \"allergens\",\n",
    "    \"ingredients_text\",\n",
    "    \"ingredients_text_en\",\n",
    "    \"countries_tags\",\n",
    "    \"brands\",\n",
    "    \"popularity_key\"\n",
    "]\n",
    "fields_param = \",\".join(fields)\n",
    "page_size = 100\n",
    "\n",
    "# Popular snack brands to prioritize\n",
    "popular_brands = [\n",
    "    \"Lay's\", \"Pringles\", \"Doritos\", \"Cheetos\", \"Oreo\", \"Kit Kat\",\n",
    "    \"Snickers\", \"M&M's\", \"Nestl√©\", \"Cadbury\", \"Hershey's\", \"Twix\",\n",
    "    \"Ruffles\", \"Tostitos\", \"Ritz\", \"Pepperidge Farm\", \"Kellogg's\"\n",
    "]\n",
    "\n",
    "print(\"‚úì API configuration ready\")\n",
    "print(\"Strategy: Malaysia snacks + Popular English-language snack brands\")\n",
    "print(f\"Fields to retrieve: {len(fields)}\")\n",
    "print(f\"Page size: {page_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uz2U69arsIOR"
   },
   "outputs": [],
   "source": [
    "# Step 5: Fetch Malaysia snacks data\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Fetching MALAYSIA SNACKS\")\n",
    "print(\"=\" * 60)\n",
    "# Code preserved unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJsz3EwksIF7"
   },
   "outputs": [],
   "source": [
    "# Step 6: Fetch popular English-language snack brands\n",
    "\n",
    "# Code preserved unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B: Data Collection and Fetching\n",
    "\n",
    "This section fetches product images and allergen information from Open Food Facts API and saves them locally for annotation and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoz3OX37sH_9"
   },
   "outputs": [],
   "source": [
    "# Step 7: Extract English-only image URLs and create annotations\n",
    "print(\"Extracting ingredient images and metadata (English only)...\")\n",
    "\n",
    "annotations = []\n",
    "used_image_urls = set()\n",
    "english_only_count = 0\n",
    "skipped_no_english = 0\n",
    "\n",
    "for product in products:\n",
    "    code = product.get(\"code\", \"\")\n",
    "    name = product.get(\"product_name\", \"\")\n",
    "    default_lang = product.get(\"lang\", \"\") or product.get(\"lc\", \"\")\n",
    "\n",
    "    # Get English ingredient text\n",
    "    ingredients_text_en = product.get(\"ingredients_text_en\", \"\")\n",
    "\n",
    "    # Skip if no English ingredients text\n",
    "    if not ingredients_text_en or ingredients_text_en.strip() == \"\":\n",
    "        skipped_no_english += 1\n",
    "        continue\n",
    "\n",
    "    ingredients_text_en = ingredients_text_en.replace(\"_\", \"\")\n",
    "\n",
    "    got_ingredient_image = False\n",
    "\n",
    "    # 1. Check for selected_images -> ingredients (prefer English)\n",
    "    if product.get(\"selected_images\") and product[\"selected_images\"].get(\"ingredients\"):\n",
    "        ingredients_images = product[\"selected_images\"][\"ingredients\"]\n",
    "        if \"display\" in ingredients_images:\n",
    "            # Priority: English, then any available\n",
    "            img_url = None\n",
    "\n",
    "            if \"en\" in ingredients_images[\"display\"]:\n",
    "                img_url = ingredients_images[\"display\"][\"en\"]\n",
    "            elif ingredients_images[\"display\"]:\n",
    "                # Take the first available if no English\n",
    "                img_url = list(ingredients_images[\"display\"].values())[0]\n",
    "\n",
    "            if img_url and (img_url not in used_image_urls):\n",
    "                used_image_urls.add(img_url)\n",
    "\n",
    "                # Get allergens\n",
    "                allergens_list = []\n",
    "                if product.get(\"allergens_tags\"):\n",
    "                    allergens_list = [\n",
    "                        tag.split(\":\")[1] if \":\" in tag else tag\n",
    "                        for tag in product[\"allergens_tags\"]\n",
    "                    ]\n",
    "                allergens_str = \", \".join(allergens_list)\n",
    "\n",
    "                filename = f\"{code}_en.jpg\"\n",
    "\n",
    "                annotations.append({\n",
    "                    \"code\": code,\n",
    "                    \"product_name\": name,\n",
    "                    \"language\": \"en\",\n",
    "                    \"allergens\": allergens_str,\n",
    "                    \"text\": ingredients_text_en,\n",
    "                    \"image_url\": img_url,\n",
    "                    \"filename\": filename\n",
    "                })\n",
    "                got_ingredient_image = True\n",
    "                english_only_count += 1\n",
    "\n",
    "    # 2. Fallback to image_ingredients_url or main image\n",
    "    if not got_ingredient_image:\n",
    "        fallback_url = None\n",
    "\n",
    "        if product.get(\"image_ingredients_url\"):\n",
    "            fallback_url = product[\"image_ingredients_url\"]\n",
    "        elif product.get(\"image_url\"):\n",
    "            fallback_url = product[\"image_url\"]\n",
    "\n",
    "        if fallback_url and (fallback_url not in used_image_urls):\n",
    "            used_image_urls.add(fallback_url)\n",
    "\n",
    "            # Get allergens\n",
    "            allergens_list = []\n",
    "            if product.get(\"allergens_tags\"):\n",
    "                allergens_list = [\n",
    "                    tag.split(\":\")[1] if \":\" in tag else tag\n",
    "                    for tag in product[\"allergens_tags\"]\n",
    "                ]\n",
    "            allergens_str = \", \".join(allergens_list)\n",
    "\n",
    "            filename = f\"{code}_en.jpg\"\n",
    "\n",
    "            annotations.append({\n",
    "                \"code\": code,\n",
    "                \"product_name\": name,\n",
    "                \"language\": \"en\",\n",
    "                \"allergens\": allergens_str,\n",
    "                \"text\": ingredients_text_en,\n",
    "                \"image_url\": fallback_url,\n",
    "                \"filename\": filename\n",
    "            })\n",
    "            english_only_count += 1\n",
    "\n",
    "print(f\"‚úì Total English images found: {len(annotations):,}\")\n",
    "print(f\"‚úì Products with English text: {english_only_count:,}\")\n",
    "print(f\"‚ö† Products skipped (no English text): {skipped_no_english:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTjVxns6sGKo"
   },
   "outputs": [],
   "source": [
    "# Step 8: Download images\n",
    "print(\"Downloading images...\")\n",
    "\n",
    "downloaded_annotations = []\n",
    "failed_downloads = []\n",
    "\n",
    "for entry in tqdm(annotations, desc=\"Downloading images\"):\n",
    "    img_url = entry[\"image_url\"]\n",
    "    filename = entry[\"filename\"]\n",
    "    save_path = os.path.join(data_dir, \"raw\", filename)\n",
    "\n",
    "    try:\n",
    "        res = requests.get(img_url, timeout=15)\n",
    "        res.raise_for_status()\n",
    "\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(res.content)\n",
    "\n",
    "        downloaded_annotations.append(entry)\n",
    "\n",
    "    except Exception as e:\n",
    "        failed_downloads.append((filename, str(e)))\n",
    "\n",
    "print(f\"\\n‚úì Images downloaded successfully: {len(downloaded_annotations):,}\")\n",
    "if failed_downloads:\n",
    "    print(f\"‚ö† Failed downloads: {len(failed_downloads)}\")\n",
    "    print(\"First 5 failures:\")\n",
    "    for filename, error in failed_downloads[:5]:\n",
    "        print(f\"  - {filename}: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section C: Quality Check and Dataset Export\n",
    "\n",
    "This section performs comprehensive quality analysis on the collected data, validates the dataset, and exports clean data for annotation and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWGJGNC3sGA_"
   },
   "outputs": [],
   "source": [
    "# Step 9: Create DataFrame and analyze data\n",
    "df = pd.DataFrame(downloaded_annotations)\n",
    "\n",
    "print(f\"‚úì Dataset created with {len(df):,} images\")\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FdyHUPjsF-K"
   },
   "outputs": [],
   "source": [
    "# Step 10: Data quality analysis\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify all entries are English\n",
    "lang_counts = df['language'].value_counts()\n",
    "print(f\"\\nüìä Language distribution:\")\n",
    "for lang, count in lang_counts.items():\n",
    "    print(f\"  {lang}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Source breakdown (Malaysia vs Popular)\n",
    "print(f\"\\nüåç Dataset composition:\")\n",
    "malaysia_count = df['code'].isin([p.get('code') for p in products_malaysia]).sum()\n",
    "popular_count = len(df) - malaysia_count\n",
    "print(f\"  Malaysia snacks: {malaysia_count:,} ({malaysia_count/len(df)*100:.1f}%)\")\n",
    "print(f\"  Popular global snacks: {popular_count:,} ({popular_count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Allergen field presence\n",
    "num_with_allergens = (df['allergens'].str.strip() != \"\").sum()\n",
    "num_without_allergens = (df['allergens'].str.strip() == \"\").sum()\n",
    "pct_with_allergens = num_with_allergens / len(df) * 100 if len(df) > 0 else 0.0\n",
    "\n",
    "print(f\"\\nüîç Allergen information:\")\n",
    "print(f\"  With allergens: {num_with_allergens:,} ({pct_with_allergens:.1f}%)\")\n",
    "print(f\"  Without allergens: {num_without_allergens:,}\")\n",
    "\n",
    "# Common allergens breakdown\n",
    "if num_with_allergens > 0:\n",
    "    print(f\"\\nü•ú Most common allergens:\")\n",
    "    all_allergens = []\n",
    "    for allergen_str in df[df['allergens'] != \"\"]['allergens']:\n",
    "        all_allergens.extend([a.strip() for a in allergen_str.split(\",\")])\n",
    "\n",
    "    allergen_counts = pd.Series(all_allergens).value_counts()\n",
    "    for allergen, count in allergen_counts.head(10).items():\n",
    "        print(f\"  {allergen}: {count:,}\")\n",
    "\n",
    "# Text field presence\n",
    "num_with_text = (df['text'].str.strip() != \"\").sum()\n",
    "avg_text_length = df[df['text'] != \"\"]['text'].str.len().mean()\n",
    "print(f\"\\nüìù Ingredient text analysis:\")\n",
    "print(f\"  With text: {num_with_text:,} ({num_with_text/len(df)*100:.1f}%)\")\n",
    "print(f\"  Average text length: {avg_text_length:.0f} characters\")\n",
    "\n",
    "# Product name presence\n",
    "num_with_name = (df['product_name'].str.strip() != \"\").sum()\n",
    "print(f\"\\nüè∑Ô∏è Product name presence:\")\n",
    "print(f\"  With name: {num_with_name:,} ({num_with_name/len(df)*100:.1f}%)\")\n",
    "print(f\"  Without name: {len(df) - num_with_name:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVrEpsHtsFom"
   },
   "outputs": [],
   "source": [
    "# Step 11: Save annotations to CSV\n",
    "csv_path = os.path.join(data_dir, \"annotations.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n‚úì Annotations saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIW4MtMUsFmB"
   },
   "outputs": [],
   "source": [
    "# Step 12: Display sample images from dataset\n",
    "from IPython.display import Image, display\n",
    "import random\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE IMAGES FROM DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show 5 random samples\n",
    "sample_size = min(5, len(df))\n",
    "sample_indices = random.sample(range(len(df)), sample_size)\n",
    "\n",
    "for i, idx in enumerate(sample_indices, 1):\n",
    "    row = df.iloc[idx]\n",
    "    print(f\"\\nüì¶ Sample {i}/{sample_size}\")\n",
    "    print(f\"Product: {row['product_name']}\")\n",
    "    print(f\"Language: {row['language']}\")\n",
    "    print(f\"Allergens: {row['allergens'] if row['allergens'] else 'None listed'}\")\n",
    "    print(f\"Text preview: {row['text'][:100]}...\" if len(row['text']) > 100 else f\"Text: {row['text']}\")\n",
    "\n",
    "    img_path = os.path.join(data_dir, \"raw\", row['filename'])\n",
    "    if os.path.exists(img_path):\n",
    "        display(Image(filename=img_path, width=400))\n",
    "    else:\n",
    "        print(\"‚ö† Image file not found\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F94gvlkbsFjc"
   },
   "outputs": [],
   "source": [
    "# Step 13: Final summary statistics and dataset evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL SUMMARY - TARGET: 10,000 IMAGES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úì Malaysia snacks: {len(products_malaysia):,}\")\n",
    "print(f\"‚úì Popular global snacks: {len(products_to_add):,}\")\n",
    "print(f\"‚úì Total products fetched: {len(products):,}\")\n",
    "print(f\"‚úì Products with English text: {english_only_count:,}\")\n",
    "print(f\"‚úì Unique English images identified: {len(annotations):,}\")\n",
    "print(f\"‚úì Images successfully downloaded: {len(downloaded_annotations):,}\")\n",
    "print(f\"‚úì All entries are in: English\")\n",
    "print(f\"‚úì Products with allergen info: {num_with_allergens:,} ({pct_with_allergens:.1f}%)\")\n",
    "print(f\"‚úì CSV file saved: {csv_path}\")\n",
    "print(f\"‚úì Raw images folder: {os.path.join(data_dir, 'raw')}\")\n",
    "\n",
    "# Dataset size evaluation against 10k target\n",
    "target = 10000\n",
    "actual = len(downloaded_annotations)\n",
    "percentage = (actual / target) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TARGET ACHIEVEMENT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Target: {target:,} images\")\n",
    "print(f\"Achieved: {actual:,} images ({percentage:.1f}% of target)\")\n",
    "\n",
    "if actual >= target:\n",
    "    print(f\"‚úì SUCCESS! Exceeded 10,000 image target\")\n",
    "elif actual >= target * 0.8:\n",
    "    print(f\"‚úì GOOD! Reached 80%+ of target - sufficient for training\")\n",
    "elif actual >= target * 0.5:\n",
    "    print(f\"‚ö†Ô∏è  FAIR: Reached 50%+ of target - usable but consider expanding\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  LOW: Below 50% of target - recommend adjusting target_total_images in Cell 6\")\n",
    "\n",
    "print(\"\\nüéâ Data collection complete!\")\n",
    "print(f\"\\n‚ÑπÔ∏è  Dataset contains:\")\n",
    "print(f\"   ‚Ä¢ Malaysian snacks (local context): {malaysia_count:,}\")\n",
    "print(f\"   ‚Ä¢ Popular international brands: {popular_count:,}\")\n",
    "print(f\"   ‚Ä¢ All with English ingredient labels\")\n",
    "print(f\"   ‚Ä¢ Brands include: Lay's, Pringles, Oreo, Kit Kat, Doritos, and more\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhS9OXI23dnu7AtKV+DuRM",
   "mount_file_id": "1DHldlqJiCzkk-CggS53H8ntsAcMTwEdN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
