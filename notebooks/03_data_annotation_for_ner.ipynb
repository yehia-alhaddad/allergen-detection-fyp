{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Data Annotation for NER\n",
    "\n",
    "Complete workflow for annotating allergen entities in product text samples to create training data for the NER model. Follow the steps sequentially to load, annotate, validate, and split data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Data Annotation for NER Training\n",
    "\n",
    "## Overview\n",
    "This notebook helps with the manual annotation process for Named Entity Recognition (NER) training data. The annotation template has been prepared by prepare_ner_sample.py and contains OCR-extracted text that needs allergen entities marked.\n",
    "\n",
    "### Purpose\n",
    "- Load the annotation template created by data preparation scripts\n",
    "- Review OCR text quality and declared allergens\n",
    "- Provide annotation guidelines and examples\n",
    "- Validate completed annotations\n",
    "- Split annotated data into train/val/test sets\n",
    "\n",
    "### Annotation Format\n",
    "Each sample needs entities marked with character positions:\n",
    "{\n",
    "    \"text\": \"Contains wheat flour, milk, and soy lecithin\",\n",
    "    \"entities\": [\n",
    "        [9, 14, \"GLUTEN\"],\n",
    "        [23, 27, \"MILK\"],\n",
    "        [33, 36, \"SOY\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "Note: This notebook is for reviewing and validating annotations. The actual annotation can be done:\n",
    "1. Manually editing the JSON file\n",
    "2. Using annotation tools (Label Studio, Doccano)\n",
    "3. Or using this notebook interactively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Load Annotation Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loading SAMPLE annotation template\n",
      "Loaded 30 samples for annotation\n",
      "File: d:\\APU Materials\\Year 3 Semester 2\\FYP\\allergen-detection-fyp\\data\\ner_training\\annotation_template_SAMPLE.json\n",
      "\n",
      "Allergen labels (12): GLUTEN, MILK, EGG, PEANUT, TREE_NUT, SOY, FISH, SHELLFISH, SESAME, MUSTARD, CELERY, SULFITES\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "NER_TRAINING = ROOT / 'data' / 'ner_training'\n",
    "\n",
    "# Check for sample annotation template\n",
    "sample_template = NER_TRAINING / 'annotation_template_SAMPLE.json'\n",
    "full_template = NER_TRAINING / 'annotation_template.json'\n",
    "\n",
    "if sample_template.exists():\n",
    "    annotation_file = sample_template\n",
    "    print(f\"✓ Loading SAMPLE annotation template\")\n",
    "elif full_template.exists():\n",
    "    annotation_file = full_template\n",
    "    print(f\"✓ Loading FULL annotation template\")\n",
    "else:\n",
    "    print(\"❌ No annotation template found!\")\n",
    "    print(f\"Run prepare_ner_sample.py or prepare_ner_training_data.py first\")\n",
    "    annotation_file = None\n",
    "\n",
    "if annotation_file:\n",
    "    with open(annotation_file, 'r', encoding='utf-8') as f:\n",
    "        annotation_data = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(annotation_data)} samples for annotation\")\n",
    "    print(f\"File: {annotation_file}\")\n",
    "    \n",
    "    # Load label mapping\n",
    "    with open(NER_TRAINING / 'label_mapping.json', 'r') as f:\n",
    "        label_mapping = json.load(f)\n",
    "    \n",
    "    allergen_labels = label_mapping['labels']\n",
    "    print(f\"\\nAllergen labels ({len(allergen_labels)}): {', '.join(allergen_labels)}\")\n",
    "else:\n",
    "    annotation_data = []\n",
    "    allergen_labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Review Sample Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE REVIEW - First 3 Samples\n",
      "================================================================================\n",
      "\n",
      "[Sample 1]\n",
      "Image: 0888849000463_en\n",
      "Product: Chocolate peanut butter protein bar, chocolate peanut butter\n",
      "Declared Allergens: milk, peanuts\n",
      "\n",
      "OCR Text (311 chars):\n",
      "  QuzsTBAR Yout 01/11/16 0r0i041cp01501} Giddn They Saio ThaT This 'Protlin BAR COULDN T MADL But We FinlLy did it\"s Delicious Food PaCKED With CPOTcN T...\n",
      "\n",
      "Entities annotated: 0\n",
      "  (No entities annotated yet)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Sample 2]\n",
      "Image: 20742690_en\n",
      "Product: Peanut Butter Wafers\n",
      "Declared Allergens: gluten, milk, peanuts, soybeans\n",
      "\n",
      "OCR Text (80 chars):\n",
      "  Peanut Butter WAFERS Joigs sGeisiot ~6-2 02 (57g) [ ) PACKAGES NETWT i202 (340g)\n",
      "\n",
      "Entities annotated: 2\n",
      "  - PEANUT: 'Peanut' (pos 0-6)\n",
      "  - TREE_NUT: 'nut' (pos 3-6)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Sample 3]\n",
      "Image: 0099482497989_en\n",
      "Product: Kettle Cooked Texas Style Barbecue Chips\n",
      "Declared Allergens: mustard\n",
      "\n",
      "OCR Text (276 chars):\n",
      "  [Nutrition Facts sentinas Seoin 10 cenngs Per conane E2 [Ziebut 12 ctizs) Ameunt Earutng Caleries 150 LDaua Total Fate9 SaturatedFang VansFat09 eaan M...\n",
      "\n",
      "Entities annotated: 1\n",
      "  - TREE_NUT: 'Nut' (pos 1-4)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display first few samples\n",
    "if annotation_data:\n",
    "    print(\"=\"*80)\n",
    "    print(\"SAMPLE REVIEW - First 3 Samples\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, sample in enumerate(annotation_data[:3], 1):\n",
    "        print(f\"\\n[Sample {i}]\")\n",
    "        print(f\"Image: {sample['image_name']}\")\n",
    "        print(f\"Product: {sample['product_name']}\")\n",
    "        print(f\"Declared Allergens: {', '.join(sample['declared_allergens'])}\")\n",
    "        print(f\"\\nOCR Text ({len(sample['text'])} chars):\")\n",
    "        print(f\"  {sample['text'][:150]}{'...' if len(sample['text']) > 150 else ''}\")\n",
    "        print(f\"\\nEntities annotated: {len(sample['entities'])}\")\n",
    "        if sample['entities']:\n",
    "            for start, end, label in sample['entities']:\n",
    "                entity_text = sample['text'][start:end]\n",
    "                print(f\"  - {label}: '{entity_text}' (pos {start}-{end})\")\n",
    "        else:\n",
    "            print(\"  (No entities annotated yet)\")\n",
    "        print(\"-\"*80)\n",
    "else:\n",
    "    print(\"No data to review. Run data preparation script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Annotation Guidelines\n",
    "\n",
    "### What to Annotate\n",
    "Mark **allergen mentions** in the OCR text with their character positions and allergen type.\n",
    "\n",
    "### Allergen Label Mapping\n",
    "- **GLUTEN**: wheat, barley, rye, oats, spelt\n",
    "- **MILK**: milk, dairy, lactose, cream, butter, cheese, whey, casein\n",
    "- **EGG** / **EGGS**: egg, eggs, albumin\n",
    "- **PEANUT** / **PEANUTS**: peanut, groundnut\n",
    "- **TREE_NUT**: almond, cashew, walnut, hazelnut, pecan, pistachio, macadamia\n",
    "- **SOY** / **SOYBEANS**: soy, soya, soybean\n",
    "- **FISH**: fish, anchovy, salmon, tuna, cod, etc.\n",
    "- **SHELLFISH**: shellfish, shrimp, crab, lobster, prawns\n",
    "- **SESAME**: sesame, sesame seeds\n",
    "- **MUSTARD**: mustard\n",
    "- **CELERY**: celery\n",
    "- **SULFITES**: sulfites, sulphites, sulfur dioxide\n",
    "\n",
    "### Annotation Format\n",
    "```python\n",
    "{\n",
    "    \"text\": \"INGREDIENTS: Wheat flour, sugar, milk powder, soy lecithin\",\n",
    "    \"entities\": [\n",
    "        (13, 18, \"GLUTEN\"),      # \"Wheat\"\n",
    "        (33, 37, \"MILK\"),         # \"milk\"\n",
    "        (47, 50, \"SOY\")           # \"soy\"\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Tips\n",
    "- Match **exact character positions** (start is inclusive, end is exclusive)\n",
    "- Use lowercase for common names, uppercase for labels\n",
    "- Mark all mentions, even if in \"may contain\" statements\n",
    "- Focus on **ingredient names**, not just \"allergens:\" labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Validate Annotations and Track Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ANNOTATION PROGRESS\n",
      "================================================================================\n",
      "\n",
      "Total samples: 30\n",
      "Annotated: 14 (46.7%)\n",
      "Not annotated: 16 (53.3%)\n",
      "\n",
      "================================================================================\n",
      "VALIDATION CHECK\n",
      "================================================================================\n",
      "✓ All annotations are valid!\n",
      "\n",
      "Entity distribution:\n",
      "  TREE_NUT       :  14\n",
      "  GLUTEN         :   5\n",
      "  PEANUT         :   1\n",
      "  SESAME         :   1\n",
      "  MILK           :   1\n"
     ]
    }
   ],
   "source": [
    "if annotation_data:\n",
    "    # Count annotation status\n",
    "    total = len(annotation_data)\n",
    "    annotated = sum(1 for s in annotation_data if s['entities'])\n",
    "    not_annotated = total - annotated\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ANNOTATION PROGRESS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal samples: {total}\")\n",
    "    print(f\"Annotated: {annotated} ({annotated/total*100:.1f}%)\")\n",
    "    print(f\"Not annotated: {not_annotated} ({not_annotated/total*100:.1f}%)\")\n",
    "    \n",
    "    if annotated > 0:\n",
    "        # Validate annotations\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"VALIDATION CHECK\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        errors = []\n",
    "        for i, sample in enumerate(annotation_data):\n",
    "            text = sample['text']\n",
    "            for start, end, label in sample['entities']:\n",
    "                # Check bounds\n",
    "                if start < 0 or end > len(text) or start >= end:\n",
    "                    errors.append(f\"Sample {i}: Invalid bounds ({start}, {end}) for text length {len(text)}\")\n",
    "                # Check label\n",
    "                if label not in allergen_labels:\n",
    "                    errors.append(f\"Sample {i}: Invalid label '{label}' (not in label set)\")\n",
    "                # Check text extraction\n",
    "                entity_text = text[start:end]\n",
    "                if not entity_text.strip():\n",
    "                    errors.append(f\"Sample {i}: Empty entity text at ({start}, {end})\")\n",
    "        \n",
    "        if errors:\n",
    "            print(f\"❌ Found {len(errors)} validation errors:\")\n",
    "            for err in errors[:10]:  # Show first 10\n",
    "                print(f\"  - {err}\")\n",
    "        else:\n",
    "            print(\"✓ All annotations are valid!\")\n",
    "            \n",
    "            # Statistics\n",
    "            entity_counts = {}\n",
    "            for sample in annotation_data:\n",
    "                for _, _, label in sample['entities']:\n",
    "                    entity_counts[label] = entity_counts.get(label, 0) + 1\n",
    "            \n",
    "            print(f\"\\nEntity distribution:\")\n",
    "            for label, count in sorted(entity_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"  {label:15s}: {count:3d}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n⚠️  No samples have been annotated yet\")\n",
    "        print(f\"\\nTo annotate:\")\n",
    "        print(f\"1. Edit file: {annotation_file}\")\n",
    "        print(f\"2. Add entities: [(start, end, 'LABEL'), ...]\")\n",
    "        print(f\"3. Re-run this notebook to validate\")\n",
    "else:\n",
    "    print(\"No annotation data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split and Save Train/Val/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA SPLIT\n",
      "================================================================================\n",
      "\n",
      "Total annotated samples: 14\n",
      "Train: 9 (64.3%)\n",
      "Val:   2 (14.3%)\n",
      "Test:  3 (21.4%)\n",
      "\n",
      "✓ Saved splits to:\n",
      "  d:\\APU Materials\\Year 3 Semester 2\\FYP\\allergen-detection-fyp\\data\\ner_training\\train.json\n",
      "  d:\\APU Materials\\Year 3 Semester 2\\FYP\\allergen-detection-fyp\\data\\ner_training\\val.json\n",
      "  d:\\APU Materials\\Year 3 Semester 2\\FYP\\allergen-detection-fyp\\data\\ner_training\\test.json\n",
      "\n",
      "✓ Ready for NER model training (Notebook 04)!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "if annotation_data:\n",
    "    # Check if we have annotated data\n",
    "    annotated_samples = [s for s in annotation_data if s['entities']]\n",
    "    \n",
    "    if len(annotated_samples) < 10:\n",
    "        print(\"⚠️  Need at least 10 annotated samples to split\")\n",
    "        print(f\"Currently have: {len(annotated_samples)} annotated samples\")\n",
    "    else:\n",
    "        # Shuffle\n",
    "        random.seed(42)\n",
    "        random.shuffle(annotated_samples)\n",
    "        \n",
    "        # Split: 70% train, 15% val, 15% test\n",
    "        n = len(annotated_samples)\n",
    "        train_split = int(0.7 * n)\n",
    "        val_split = int(0.85 * n)\n",
    "        \n",
    "        train_data = annotated_samples[:train_split]\n",
    "        val_data = annotated_samples[train_split:val_split]\n",
    "        test_data = annotated_samples[val_split:]\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"DATA SPLIT\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nTotal annotated samples: {n}\")\n",
    "        print(f\"Train: {len(train_data)} ({len(train_data)/n*100:.1f}%)\")\n",
    "        print(f\"Val:   {len(val_data)} ({len(val_data)/n*100:.1f}%)\")\n",
    "        print(f\"Test:  {len(test_data)} ({len(test_data)/n*100:.1f}%)\")\n",
    "        \n",
    "        # Save splits\n",
    "        with open(NER_TRAINING / 'train.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        with open(NER_TRAINING / 'val.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(val_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        with open(NER_TRAINING / 'test.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\n✓ Saved splits to:\")\n",
    "        print(f\"  {NER_TRAINING / 'train.json'}\")\n",
    "        print(f\"  {NER_TRAINING / 'val.json'}\")\n",
    "        print(f\"  {NER_TRAINING / 'test.json'}\")\n",
    "        print(f\"\\n✓ Ready for NER model training (Notebook 04)!\")\n",
    "else:\n",
    "    print(\"No annotation data to split\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOAOO6C8D31RJ7Xf0iUk5Tc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
