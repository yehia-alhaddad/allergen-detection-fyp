{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 06: Integration Experiments\n",
    "\n",
    "This notebook integrates all components into a complete end-to-end allergen detection pipeline and tests the system comprehensively.\n",
    "\n",
    "**Pipeline Components:**\n",
    "- SimpleOCREngine (text extraction)\n",
    "- Trained NER Model (allergen entity recognition)\n",
    "- Allergen Dictionary (synonym mapping)\n",
    "- Detection confidence scoring\n",
    "\n",
    "**Experiments:**\n",
    "1. End-to-end pipeline integration\n",
    "2. Sample image testing\n",
    "3. Error analysis and categorization\n",
    "4. Performance optimization\n",
    "5. Batch processing benchmarks\n",
    "6. Comprehensive detection reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**Required:**\n",
    "- Completed Notebook 02 (OCR preprocessing)\n",
    "- Completed Notebook 04 (NER model training)\n",
    "- Trained model in `models/ner_model/`\n",
    "- Test images in `data/raw/` or `data/ocr_results/test_samples/`\n",
    "\n",
    "**Dependencies:**\n",
    "- transformers, torch, easyocr, cv2, pandas, numpy\n",
    "\n",
    "Run cells in sequential order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A: Setup and Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setup paths and environment\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Detect environment and set paths\n",
    "ROOT = Path.cwd()\n",
    "if ROOT.name == \"notebooks\":\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "SRC = ROOT / \"src\"\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "MODELS_DIR = ROOT / \"models\"\n",
    "RESULTS_DIR = ROOT / \"results\"\n",
    "\n",
    "# Add src to path for imports\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "\n",
    "print(\"‚úì Environment setup complete\")\n",
    "print(f\"Root directory: {ROOT}\")\n",
    "print(f\"Source code: {SRC}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Import required libraries\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from typing import List, Dict, Tuple\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Local imports\n",
    "from ocr.simple_ocr_engine import SimpleOCREngine\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load All Pipeline Components\n",
    "\n",
    "Load the trained NER model, OCR engine, allergen dictionary, and configure the complete detection pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3a: Load allergen dictionary and label mapping\n",
    "\n",
    "# Load allergen dictionary\n",
    "allergen_dict_path = DATA_DIR / \"allergen_dictionary.json\"\n",
    "with open(allergen_dict_path, 'r') as f:\n",
    "    allergen_dictionary = json.load(f)\n",
    "\n",
    "print(f\"‚úì Loaded allergen dictionary with {len(allergen_dictionary)} allergen types\")\n",
    "print(f\"  Allergen types: {', '.join(allergen_dictionary.keys())}\")\n",
    "\n",
    "# Load label mapping\n",
    "label_mapping_path = DATA_DIR / \"ner_training\" / \"label_mapping.json\"\n",
    "with open(label_mapping_path, 'r') as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "id2label = {int(k): v for k, v in label_mapping[\"id2label\"].items()}\n",
    "label2id = {v: int(k) for k, v in label_mapping[\"label2id\"].items()}\n",
    "\n",
    "print(f\"‚úì Loaded label mapping with {len(id2label)} labels\")\n",
    "print(f\"  Labels: {list(id2label.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3b: Load trained NER model and tokenizer\n",
    "\n",
    "model_path = MODELS_DIR / \"ner_model\"\n",
    "\n",
    "if not model_path.exists():\n",
    "    print(\"‚ö†Ô∏è  Model not found at models/ner_model/\")\n",
    "    print(\"   Checking experiments folder...\")\n",
    "    experiment_models = list((MODELS_DIR / \"experiments\").glob(\"**/pytorch_model.bin\"))\n",
    "    if experiment_models:\n",
    "        model_path = experiment_models[0].parent\n",
    "        print(f\"   Using model from: {model_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No trained model found. Please run Notebook 04 first.\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"‚úì Loaded NER model from: {model_path}\")\n",
    "    print(f\"  Model device: {device}\")\n",
    "    print(f\"  Number of labels: {model.config.num_labels}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3c: Initialize OCR engine\n",
    "\n",
    "try:\n",
    "    ocr_engine = SimpleOCREngine(lang_list=[\"en\"], gpu=torch.cuda.is_available())\n",
    "    print(\"‚úì OCR engine initialized\")\n",
    "    print(f\"  Languages: English\")\n",
    "    print(f\"  GPU enabled: {torch.cuda.is_available()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error initializing OCR: {e}\")\n",
    "    print(\"  OCR will be skipped, using ground truth text instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B: Build Integration Pipeline\n",
    "\n",
    "Create the complete end-to-end allergen detection pipeline that integrates all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define helper functions for text processing and NER\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize extracted text.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Remove underscores (common OCR artifact)\n",
    "    text = text.replace('_', '')\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def run_ner_prediction(text: str, tokenizer, model, device) -> List[Tuple[str, str, float]]:\n",
    "    \"\"\"\n",
    "    Run NER model on text and return detected entities.\n",
    "    \n",
    "    Returns:\n",
    "        List of (entity_text, label, confidence) tuples\n",
    "    \"\"\"\n",
    "    if not text or len(text.strip()) < 3:\n",
    "        return []\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=True,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")[0].numpy()\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        confidences = probs.max(dim=-1).values.cpu().numpy()\n",
    "    \n",
    "    # Extract entities\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    current_label = None\n",
    "    current_start = None\n",
    "    current_end = None\n",
    "    current_conf = []\n",
    "    \n",
    "    for idx, (pred, conf, (start, end)) in enumerate(zip(predictions, confidences, offset_mapping)):\n",
    "        # Skip special tokens\n",
    "        if start == end:\n",
    "            continue\n",
    "        \n",
    "        label = id2label[pred]\n",
    "        \n",
    "        # Check if this is an allergen entity\n",
    "        if label.startswith('B-'):\n",
    "            # Save previous entity if exists\n",
    "            if current_entity:\n",
    "                entities.append((\n",
    "                    current_entity,\n",
    "                    current_label,\n",
    "                    float(np.mean(current_conf))\n",
    "                ))\n",
    "            \n",
    "            # Start new entity\n",
    "            current_entity = text[start:end]\n",
    "            current_label = label[2:]  # Remove B- prefix\n",
    "            current_start = start\n",
    "            current_end = end\n",
    "            current_conf = [conf]\n",
    "            \n",
    "        elif label.startswith('I-') and current_entity:\n",
    "            # Continue current entity\n",
    "            current_end = end\n",
    "            current_entity = text[current_start:current_end]\n",
    "            current_conf.append(conf)\n",
    "        \n",
    "        else:\n",
    "            # Not an entity - save current if exists\n",
    "            if current_entity:\n",
    "                entities.append((\n",
    "                    current_entity,\n",
    "                    current_label,\n",
    "                    float(np.mean(current_conf))\n",
    "                ))\n",
    "                current_entity = None\n",
    "                current_label = None\n",
    "                current_conf = []\n",
    "    \n",
    "    # Don't forget last entity\n",
    "    if current_entity:\n",
    "        entities.append((\n",
    "            current_entity,\n",
    "            current_label,\n",
    "            float(np.mean(current_conf))\n",
    "        ))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "\n",
    "def map_to_standard_allergens(entities: List[Tuple[str, str, float]], allergen_dict: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Map detected entities to standard allergen categories.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with detected allergens, confidence, and details\n",
    "    \"\"\"\n",
    "    detected_allergens = defaultdict(list)\n",
    "    \n",
    "    for entity_text, label, confidence in entities:\n",
    "        entity_lower = entity_text.lower().strip()\n",
    "        \n",
    "        # Try to match against dictionary\n",
    "        matched = False\n",
    "        for allergen_type, synonyms in allergen_dict.items():\n",
    "            for synonym in synonyms:\n",
    "                if synonym.lower() in entity_lower or entity_lower in synonym.lower():\n",
    "                    detected_allergens[allergen_type].append({\n",
    "                        'text': entity_text,\n",
    "                        'confidence': confidence,\n",
    "                        'label': label\n",
    "                    })\n",
    "                    matched = True\n",
    "                    break\n",
    "            if matched:\n",
    "                break\n",
    "        \n",
    "        # If no match, still include under \"unknown\"\n",
    "        if not matched and label != 'O':\n",
    "            detected_allergens['unknown'].append({\n",
    "                'text': entity_text,\n",
    "                'confidence': confidence,\n",
    "                'label': label\n",
    "            })\n",
    "    \n",
    "    return dict(detected_allergens)\n",
    "\n",
    "print(\"‚úì Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build complete end-to-end pipeline\n",
    "\n",
    "def detect_allergens_from_image(\n",
    "    image_path: str,\n",
    "    use_ocr: bool = True,\n",
    "    ground_truth_text: str = None,\n",
    "    verbose: bool = False\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Complete end-to-end allergen detection pipeline.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to product image\n",
    "        use_ocr: Whether to run OCR (False = use ground_truth_text)\n",
    "        ground_truth_text: Ground truth text if use_ocr=False\n",
    "        verbose: Print detailed progress\n",
    "    \n",
    "    Returns:\n",
    "        Dict with detection results, timing, and confidence\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'image_path': str(image_path),\n",
    "        'success': False,\n",
    "        'error': None,\n",
    "        'timings': {},\n",
    "        'raw_text': '',\n",
    "        'cleaned_text': '',\n",
    "        'entities_found': [],\n",
    "        'detected_allergens': {},\n",
    "        'total_allergens': 0,\n",
    "        'avg_confidence': 0.0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Extract text (OCR or ground truth)\n",
    "        t0 = time.time()\n",
    "        \n",
    "        if use_ocr and 'ocr_engine' in globals():\n",
    "            if verbose:\n",
    "                print(f\"Running OCR on {Path(image_path).name}...\")\n",
    "            \n",
    "            img = cv2.imread(str(image_path))\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Could not load image: {image_path}\")\n",
    "            \n",
    "            raw_text = ocr_engine.extract(img)\n",
    "            results['timings']['ocr'] = time.time() - t0\n",
    "            \n",
    "        else:\n",
    "            if ground_truth_text is None:\n",
    "                raise ValueError(\"use_ocr=False requires ground_truth_text\")\n",
    "            raw_text = ground_truth_text\n",
    "            results['timings']['ocr'] = 0.0\n",
    "        \n",
    "        results['raw_text'] = raw_text\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Text extracted ({len(raw_text)} chars)\")\n",
    "        \n",
    "        # Step 2: Clean text\n",
    "        t0 = time.time()\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        results['cleaned_text'] = cleaned_text\n",
    "        results['timings']['cleaning'] = time.time() - t0\n",
    "        \n",
    "        if not cleaned_text or len(cleaned_text) < 3:\n",
    "            results['error'] = 'Text too short or empty'\n",
    "            return results\n",
    "        \n",
    "        # Step 3: Run NER prediction\n",
    "        t0 = time.time()\n",
    "        entities = run_ner_prediction(cleaned_text, tokenizer, model, device)\n",
    "        results['entities_found'] = entities\n",
    "        results['timings']['ner'] = time.time() - t0\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Found {len(entities)} entities\")\n",
    "        \n",
    "        # Step 4: Map to standard allergens\n",
    "        t0 = time.time()\n",
    "        detected_allergens = map_to_standard_allergens(entities, allergen_dictionary)\n",
    "        results['detected_allergens'] = detected_allergens\n",
    "        results['timings']['mapping'] = time.time() - t0\n",
    "        \n",
    "        # Step 5: Calculate summary statistics\n",
    "        all_confidences = []\n",
    "        for allergen_type, detections in detected_allergens.items():\n",
    "            for det in detections:\n",
    "                all_confidences.append(det['confidence'])\n",
    "        \n",
    "        results['total_allergens'] = len(detected_allergens)\n",
    "        results['avg_confidence'] = float(np.mean(all_confidences)) if all_confidences else 0.0\n",
    "        results['total_time'] = sum(results['timings'].values())\n",
    "        results['success'] = True\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Detected {results['total_allergens']} allergen types\")\n",
    "            print(f\"  Total time: {results['total_time']:.3f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['error'] = str(e)\n",
    "        if verbose:\n",
    "            print(f\"  ‚ùå Error: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úì End-to-end pipeline function defined\")\n",
    "print(\"  Usage: detect_allergens_from_image(image_path, use_ocr=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section C: Test on Sample Images\n",
    "\n",
    "Test the pipeline on sample images and analyze results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Load test samples\n",
    "\n",
    "# Look for test samples in multiple locations\n",
    "test_samples_dir = DATA_DIR / \"ocr_results\" / \"test_samples\"\n",
    "raw_images_dir = DATA_DIR / \"raw\"\n",
    "\n",
    "test_images = []\n",
    "\n",
    "if test_samples_dir.exists():\n",
    "    test_images.extend(list(test_samples_dir.glob(\"*.jpg\")))\n",
    "    test_images.extend(list(test_samples_dir.glob(\"*.png\")))\n",
    "\n",
    "if len(test_images) < 10 and raw_images_dir.exists():\n",
    "    raw_imgs = list(raw_images_dir.glob(\"*.jpg\"))\n",
    "    test_images.extend(raw_imgs[:min(20, len(raw_imgs))])\n",
    "\n",
    "# Remove duplicates\n",
    "test_images = list(set(test_images))\n",
    "\n",
    "print(f\"‚úì Found {len(test_images)} test images\")\n",
    "if test_images:\n",
    "    print(f\"  Sample paths:\")\n",
    "    for img in test_images[:5]:\n",
    "        print(f\"    - {img.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Run pipeline on sample images\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNNING PIPELINE ON SAMPLE IMAGES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test on first 10 images (or fewer if less available)\n",
    "n_samples = min(10, len(test_images))\n",
    "sample_results = []\n",
    "\n",
    "for i, img_path in enumerate(tqdm(test_images[:n_samples], desc=\"Processing images\"), 1):\n",
    "    print(f\"\\nüì¶ Sample {i}/{n_samples}: {img_path.name}\")\n",
    "    \n",
    "    # Check if ground truth text exists\n",
    "    txt_path = img_path.with_suffix('.txt')\n",
    "    ground_truth_text = None\n",
    "    \n",
    "    if txt_path.exists():\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            ground_truth_text = f.read()\n",
    "    \n",
    "    # Run pipeline\n",
    "    result = detect_allergens_from_image(\n",
    "        img_path,\n",
    "        use_ocr=('ocr_engine' in globals()),\n",
    "        ground_truth_text=ground_truth_text,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    sample_results.append(result)\n",
    "    \n",
    "    # Print summary\n",
    "    if result['success']:\n",
    "        print(f\"  ‚úì Success!\")\n",
    "        print(f\"    Text: {result['cleaned_text'][:100]}...\")\n",
    "        print(f\"    Allergens detected: {result['total_allergens']}\")\n",
    "        if result['detected_allergens']:\n",
    "            for allergen_type, detections in result['detected_allergens'].items():\n",
    "                print(f\"      - {allergen_type}: {[d['text'] for d in detections]}\")\n",
    "        print(f\"    Avg confidence: {result['avg_confidence']:.2%}\")\n",
    "        print(f\"    Total time: {result['total_time']:.3f}s\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Failed: {result['error']}\")\n",
    "\n",
    "print(f\"\\n‚úì Processed {n_samples} samples\")\n",
    "print(f\"  Success rate: {sum(r['success'] for r in sample_results)}/{n_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section D: Error Analysis and Categorization\n",
    "\n",
    "Analyze failures and categorize error types to identify improvement areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Categorize errors and analyze failure patterns\n",
    "\n",
    "error_categories = {\n",
    "    'ocr_failure': [],      # OCR couldn't extract text\n",
    "    'ner_failure': [],      # NER didn't detect entities\n",
    "    'mapping_failure': [],  # Entities found but not mapped\n",
    "    'low_confidence': [],   # Detection but low confidence\n",
    "    'success': []           # Successful detection\n",
    "}\n",
    "\n",
    "for result in sample_results:\n",
    "    img_name = Path(result['image_path']).name\n",
    "    \n",
    "    if not result['success']:\n",
    "        if 'Text too short' in str(result['error']):\n",
    "            error_categories['ocr_failure'].append(img_name)\n",
    "        else:\n",
    "            error_categories['ocr_failure'].append(img_name)\n",
    "    \n",
    "    elif result['total_allergens'] == 0:\n",
    "        if len(result['entities_found']) == 0:\n",
    "            error_categories['ner_failure'].append(img_name)\n",
    "        else:\n",
    "            error_categories['mapping_failure'].append(img_name)\n",
    "    \n",
    "    elif result['avg_confidence'] < 0.5:\n",
    "        error_categories['low_confidence'].append(img_name)\n",
    "    \n",
    "    else:\n",
    "        error_categories['success'].append(img_name)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for category, items in error_categories.items():\n",
    "    print(f\"\\n{category.upper().replace('_', ' ')}: {len(items)}\")\n",
    "    if items and len(items) <= 5:\n",
    "        for item in items:\n",
    "            print(f\"  - {item}\")\n",
    "    elif items:\n",
    "        print(f\"  - {items[0]}\")\n",
    "        print(f\"  - {items[1]}\")\n",
    "        print(f\"  ... and {len(items) - 2} more\")\n",
    "\n",
    "# Calculate statistics\n",
    "total_samples = len(sample_results)\n",
    "success_rate = len(error_categories['success']) / total_samples if total_samples > 0 else 0\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"SUMMARY STATISTICS\")\n",
    "print(f\"{'=' * 70}\")\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "print(f\"Success rate: {success_rate:.1%}\")\n",
    "print(f\"OCR failures: {len(error_categories['ocr_failure'])} ({len(error_categories['ocr_failure'])/total_samples:.1%})\")\n",
    "print(f\"NER failures: {len(error_categories['ner_failure'])} ({len(error_categories['ner_failure'])/total_samples:.1%})\")\n",
    "print(f\"Mapping failures: {len(error_categories['mapping_failure'])} ({len(error_categories['mapping_failure'])/total_samples:.1%})\")\n",
    "print(f\"Low confidence: {len(error_categories['low_confidence'])} ({len(error_categories['low_confidence'])/total_samples:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section E: Performance Benchmarking\n",
    "\n",
    "Measure pipeline performance metrics including timing, throughput, and resource usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Analyze pipeline performance and bottlenecks\n",
    "\n",
    "# Extract timing data\n",
    "timing_data = {\n",
    "    'ocr': [],\n",
    "    'cleaning': [],\n",
    "    'ner': [],\n",
    "    'mapping': [],\n",
    "    'total': []\n",
    "}\n",
    "\n",
    "for result in sample_results:\n",
    "    if result['success'] and 'timings' in result:\n",
    "        for key in timing_data.keys():\n",
    "            if key == 'total':\n",
    "                timing_data[key].append(result.get('total_time', 0))\n",
    "            else:\n",
    "                timing_data[key].append(result['timings'].get(key, 0))\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PERFORMANCE BENCHMARKS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for component, times in timing_data.items():\n",
    "    if times:\n",
    "        avg_time = np.mean(times)\n",
    "        std_time = np.std(times)\n",
    "        min_time = np.min(times)\n",
    "        max_time = np.max(times)\n",
    "        \n",
    "        print(f\"\\n{component.upper()}:\")\n",
    "        print(f\"  Average: {avg_time:.3f}s\")\n",
    "        print(f\"  Std Dev: {std_time:.3f}s\")\n",
    "        print(f\"  Min: {min_time:.3f}s\")\n",
    "        print(f\"  Max: {max_time:.3f}s\")\n",
    "        \n",
    "        if component != 'total':\n",
    "            total_avg = np.mean(timing_data['total'])\n",
    "            pct = (avg_time / total_avg * 100) if total_avg > 0 else 0\n",
    "            print(f\"  % of total: {pct:.1f}%\")\n",
    "\n",
    "# Calculate throughput\n",
    "if timing_data['total']:\n",
    "    avg_total_time = np.mean(timing_data['total'])\n",
    "    throughput = 1.0 / avg_total_time if avg_total_time > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"THROUGHPUT\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"Average time per image: {avg_total_time:.3f}s\")\n",
    "    print(f\"Images per second: {throughput:.2f}\")\n",
    "    print(f\"Images per minute: {throughput * 60:.1f}\")\n",
    "    print(f\"Estimated time for 1000 images: {(avg_total_time * 1000) / 60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Visualize performance breakdown\n",
    "\n",
    "if timing_data['ocr']:\n",
    "    # Calculate average time for each component\n",
    "    component_times = {\n",
    "        'OCR': np.mean(timing_data['ocr']),\n",
    "        'Cleaning': np.mean(timing_data['cleaning']),\n",
    "        'NER': np.mean(timing_data['ner']),\n",
    "        'Mapping': np.mean(timing_data['mapping'])\n",
    "    }\n",
    "    \n",
    "    # Create pie chart\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Pie chart of time distribution\n",
    "    colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
    "    ax1.pie(\n",
    "        component_times.values(),\n",
    "        labels=component_times.keys(),\n",
    "        autopct='%1.1f%%',\n",
    "        colors=colors,\n",
    "        startangle=90\n",
    "    )\n",
    "    ax1.set_title('Pipeline Time Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Bar chart of absolute times\n",
    "    ax2.bar(component_times.keys(), component_times.values(), color=colors)\n",
    "    ax2.set_ylabel('Time (seconds)', fontsize=12)\n",
    "    ax2.set_title('Average Time per Component', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Performance visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section F: Comprehensive Detection Report\n",
    "\n",
    "Generate detailed reports with visualizations and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Generate comprehensive detection report\n",
    "\n",
    "# Aggregate allergen detection statistics\n",
    "allergen_stats = defaultdict(int)\n",
    "confidence_by_allergen = defaultdict(list)\n",
    "\n",
    "for result in sample_results:\n",
    "    if result['success'] and result['detected_allergens']:\n",
    "        for allergen_type, detections in result['detected_allergens'].items():\n",
    "            allergen_stats[allergen_type] += 1\n",
    "            for det in detections:\n",
    "                confidence_by_allergen[allergen_type].append(det['confidence'])\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ALLERGEN DETECTION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nTotal images processed: {len(sample_results)}\")\n",
    "print(f\"Images with allergens detected: {sum(1 for r in sample_results if r.get('total_allergens', 0) > 0)}\")\n",
    "\n",
    "print(f\"\\n{'Allergen Type':<20} {'Count':<10} {'Avg Confidence':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for allergen_type in sorted(allergen_stats.keys()):\n",
    "    count = allergen_stats[allergen_type]\n",
    "    avg_conf = np.mean(confidence_by_allergen[allergen_type])\n",
    "    print(f\"{allergen_type:<20} {count:<10} {avg_conf:.2%}\")\n",
    "\n",
    "# Visualize allergen distribution\n",
    "if allergen_stats:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    allergen_types = list(allergen_stats.keys())\n",
    "    counts = [allergen_stats[at] for at in allergen_types]\n",
    "    \n",
    "    bars = ax.barh(allergen_types, counts, color='skyblue')\n",
    "    ax.set_xlabel('Number of Detections', fontsize=12)\n",
    "    ax.set_title('Allergen Detection Frequency', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2, \n",
    "                f' {count}', ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\\\n‚úì Allergen distribution visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Save detailed results to JSON\n",
    "\n",
    "# Prepare results directory\n",
    "integration_results_dir = RESULTS_DIR / \"integration_experiments\"\n",
    "integration_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create timestamp for this run\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Prepare results summary\n",
    "results_summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'total_samples': len(sample_results),\n",
    "    'successful_detections': sum(1 for r in sample_results if r['success'] and r['total_allergens'] > 0),\n",
    "    'failed_samples': sum(1 for r in sample_results if not r['success']),\n",
    "    'error_categories': {k: len(v) for k, v in error_categories.items()},\n",
    "    'allergen_statistics': dict(allergen_stats),\n",
    "    'average_confidence': float(np.mean([r['avg_confidence'] for r in sample_results if r['success']])) if any(r['success'] for r in sample_results) else 0.0,\n",
    "    'performance_metrics': {\n",
    "        'avg_total_time': float(np.mean(timing_data['total'])) if timing_data['total'] else 0.0,\n",
    "        'avg_ocr_time': float(np.mean(timing_data['ocr'])) if timing_data['ocr'] else 0.0,\n",
    "        'avg_ner_time': float(np.mean(timing_data['ner'])) if timing_data['ner'] else 0.0,\n",
    "        'throughput_per_second': float(1.0 / np.mean(timing_data['total'])) if timing_data['total'] and np.mean(timing_data['total']) > 0 else 0.0\n",
    "    },\n",
    "    'detailed_results': sample_results\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "output_path = integration_results_dir / f\"integration_results_{timestamp}.json\"\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úì Results saved to: {output_path}\")\n",
    "print(f\"  Total size: {output_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section G: Optional - Batch Processing Test\n",
    "\n",
    "Run pipeline on larger batch for comprehensive metrics (optional - can be slow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Batch processing test (OPTIONAL - set ENABLE_BATCH = True to run)\n",
    "\n",
    "ENABLE_BATCH = True   # Set to True to test on larger batch\n",
    "BATCH_SIZE = 25       # Number of images to process in batch (reasonable for testing)\n",
    "\n",
    "if ENABLE_BATCH and len(test_images) > n_samples:\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"BATCH PROCESSING TEST - {BATCH_SIZE} IMAGES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    batch_images = test_images[n_samples:n_samples + BATCH_SIZE]\n",
    "    batch_results = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for img_path in tqdm(batch_images, desc=\"Batch processing\"):\n",
    "        # Check for ground truth\n",
    "        txt_path = img_path.with_suffix('.txt')\n",
    "        ground_truth = None\n",
    "        if txt_path.exists():\n",
    "            with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                ground_truth = f.read()\n",
    "        \n",
    "        # Run pipeline\n",
    "        result = detect_allergens_from_image(\n",
    "            img_path,\n",
    "            use_ocr=('ocr_engine' in globals()),\n",
    "            ground_truth_text=ground_truth,\n",
    "            verbose=False\n",
    "        )\n",
    "        batch_results.append(result)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate batch metrics\n",
    "    batch_success = sum(1 for r in batch_results if r['success'])\n",
    "    batch_allergens_detected = sum(1 for r in batch_results if r.get('total_allergens', 0) > 0)\n",
    "    batch_avg_confidence = np.mean([r['avg_confidence'] for r in batch_results if r['success']])\n",
    "    \n",
    "    print(f\"\\n‚úì Batch processing complete\")\n",
    "    print(f\"  Total time: {total_time:.2f}s\")\n",
    "    print(f\"  Avg time per image: {total_time/len(batch_images):.3f}s\")\n",
    "    print(f\"  Success rate: {batch_success/len(batch_images):.1%}\")\n",
    "    print(f\"  Detection rate: {batch_allergens_detected/len(batch_images):.1%}\")\n",
    "    print(f\"  Avg confidence: {batch_avg_confidence:.2%}\")\n",
    "    \n",
    "    # Save batch results\n",
    "    batch_output_path = integration_results_dir / f\"batch_results_{timestamp}.json\"\n",
    "    with open(batch_output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(batch_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"  Batch results saved to: {batch_output_path}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Batch processing disabled. Set ENABLE_BATCH = True to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook successfully integrated all pipeline components and tested the end-to-end allergen detection system.\n",
    "\n",
    "**Key Findings:**\n",
    "- Pipeline successfully processes images from OCR ‚Üí NER ‚Üí Allergen Detection\n",
    "- Performance metrics collected for optimization\n",
    "- Error patterns identified for improvement\n",
    "\n",
    "**Next Steps:**\n",
    "1. **Optimization:** Address bottlenecks identified in performance analysis\n",
    "2. **Error Reduction:** Improve OCR quality for failed cases\n",
    "3. **Confidence Tuning:** Adjust thresholds based on confidence analysis\n",
    "4. **Scale Testing:** Run on full dataset (Notebook 07: App Interface)\n",
    "5. **Deployment:** Package pipeline for production use\n",
    "\n",
    "**Outputs Generated:**\n",
    "- `results/integration_experiments/integration_results_*.json` - Detailed results\n",
    "- Performance visualizations and metrics\n",
    "- Error analysis and categorization\n",
    "\n",
    "**Usage for New Images:**\n",
    "```python\n",
    "result = detect_allergens_from_image(\"path/to/image.jpg\", use_ocr=True)\n",
    "print(result['detected_allergens'])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOAOO6C8D31RJ7Xf0iUk5Tc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
