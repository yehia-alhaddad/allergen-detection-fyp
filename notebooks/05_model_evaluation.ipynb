{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure:\n",
    "1. Notebook 03: Complete data annotation and splits → `data/ner_training/{train, val, test}.json`\n",
    "2. Notebook 04: Train NER model → `models/ner_model/` or `models/experiments/ner_model/`\n",
    "\n",
    "Run cells in order. Each cell is independent after Step 1–3 complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Order and Requirements\n",
    "1. Complete annotation + split in Notebook 03 (produces train.json, val.json, test.json).\n",
    "2. Train the NER model in Notebook 04 (saves to models/ner_model).\n",
    "3. Run this notebook to evaluate on val/test and save metrics.\n",
    "\n",
    "Requirements:\n",
    "- Files: data/ner_training/label_mapping.json, val.json/test.json\n",
    "- Model directory: models/ner_model (or fallback: models/experiments/ner_model)\n",
    "- Packages: transformers, datasets, seqeval, numpy, pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1qZevqJWSCJ"
   },
   "source": [
    "# Notebook 05: NER Model Evaluation\n",
    "\n",
    "Load a trained NER model and evaluate on validation/test splits.\n",
    "\n",
    "**Outputs:**\n",
    "- Overall micro metrics: precision, recall, F1\n",
    "- Per-label classification report (seqeval)\n",
    "- Qualitative examples: gold vs. predicted entities\n",
    "- Metrics JSON saved to results/model_metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Environment and paths setup\n",
    "import sys, json, random, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect workspace root\n",
    "ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "SRC = ROOT / 'src'\n",
    "\n",
    "# Add src to path for local imports\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.append(str(SRC))\n",
    "    print(f\"Added to sys.path: {SRC}\")\n",
    "\n",
    "# Define expected paths\n",
    "NER_TRAINING = ROOT / 'data' / 'ner_training'\n",
    "MODELS_PREFERRED = ROOT / 'models' / 'ner_model'\n",
    "MODELS_FALLBACK = ROOT / 'models' / 'experiments' / 'ner_model'\n",
    "RESULTS_DIR = ROOT / 'results' / 'model_metrics'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "val_path = NER_TRAINING / 'val.json'\n",
    "test_path = NER_TRAINING / 'test.json'\n",
    "label_map_path = NER_TRAINING / 'label_mapping.json'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PATHS DETECTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ROOT:           {ROOT}\")\n",
    "print(f\"NER_TRAINING:   {NER_TRAINING.exists()}\")\n",
    "print(f\"val.json:       {val_path.exists()}\")\n",
    "print(f\"test.json:      {test_path.exists()}\")\n",
    "print(f\"label_mapping:  {label_map_path.exists()}\")\n",
    "print(f\"RESULTS_DIR:    {RESULTS_DIR.exists()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    import json\n",
    "    import random\n",
    "    from typing import List, Tuple, Dict, Any\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    import transformers\n",
    "    from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "    from datasets import Dataset\n",
    "    import seqeval\n",
    "    from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "    print(\"Libraries imported OK\")\n",
    "    print(f\"  transformers v{transformers.__version__}\")\n",
    "    print(f\"  seqeval v{seqeval.__version__}\")\n",
    "    print()\n",
    "except ImportError as e:\n",
    "    print(\"ERROR: Missing package.\")\n",
    "    print(f\"  {e}\")\n",
    "    print(\"Install with: pip install transformers datasets seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load label mapping and data splits\n",
    "def load_json(path: Path):\n",
    "    if path.exists():\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "print(\"Loading label mapping and data splits...\")\n",
    "label_map_path = NER_TRAINING / 'label_mapping.json'\n",
    "mapping = load_json(label_map_path)\n",
    "\n",
    "if not mapping:\n",
    "    print(\"WARNING: label_mapping.json not found\", label_map_path)\n",
    "    labels = []\n",
    "else:\n",
    "    labels = mapping.get('labels', [])\n",
    "    print(f\"Loaded {len(labels)} allergen labels\")\n",
    "\n",
    "val_data = load_json(val_path)\n",
    "test_data = load_json(test_path)\n",
    "\n",
    "print(f\"val.json:  {len(val_data) if val_data else 0} samples\")\n",
    "print(f\"test.json: {len(test_data) if test_data else 0} samples\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Utility functions for tokenization and evaluation\n",
    "\n",
    "def tokenize_and_align_labels(samples, tokenizer, label2id):\n",
    "    \"\"\"Align word-level labels to token-level (BIO tags).\"\"\"\n",
    "    tokenized = tokenizer(samples['tokens'], is_split_into_words=True, truncation=True)\n",
    "    labels = []\n",
    "    for i, label_seq in enumerate(samples['labels']):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        prev_word = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                label_ids.append(-100)  # Skip special tokens\n",
    "            elif wid != prev_word:\n",
    "                label_ids.append(label2id.get(label_seq[wid], 0))\n",
    "            else:\n",
    "                label_ids.append(-100)  # Skip sub-tokens\n",
    "            prev_word = wid\n",
    "        labels.append(label_ids)\n",
    "    tokenized['labels'] = labels\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def decode_predictions(logits, label_list):\n",
    "    \"\"\"Decode model logits to label sequences.\"\"\"\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return [[label_list[p] if p < len(label_list) else 'O' for p in seq] for seq in preds]\n",
    "\n",
    "\n",
    "def group_tokens_to_entities(tokens, labels):\n",
    "    \"\"\"Convert BIO tags back to (start_idx, end_idx, label) tuples.\"\"\"\n",
    "    entities = []\n",
    "    current_label = None\n",
    "    start_idx = None\n",
    "    for i, (tok, lab) in enumerate(zip(tokens, labels)):\n",
    "        if lab.startswith('B-'):\n",
    "            if current_label is not None:\n",
    "                entities.append((start_idx, i, current_label))\n",
    "            current_label = lab[2:]\n",
    "            start_idx = i\n",
    "        elif lab.startswith('I-') and current_label == lab[2:]:\n",
    "            continue\n",
    "        else:\n",
    "            if current_label is not None:\n",
    "                entities.append((start_idx, i, current_label))\n",
    "                current_label = None\n",
    "                start_idx = None\n",
    "    if current_label is not None:\n",
    "        entities.append((start_idx, len(tokens), current_label))\n",
    "    return entities\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, label_list):\n",
    "    \"\"\"Compute seqeval metrics on a dataset.\"\"\"\n",
    "    label2id = {l: i for i, l in enumerate(label_list)}\n",
    "    tokenized = dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer, label2id), batched=True)\n",
    "    tokenized = tokenized.remove_columns([c for c in tokenized.column_names if c not in ['input_ids', 'attention_mask', 'labels']])\n",
    "    \n",
    "    import torch\n",
    "    model.eval()\n",
    "    all_preds, all_refs = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(tokenized), 8):\n",
    "            batch = tokenized[i:i+8]\n",
    "            inputs = {\n",
    "                'input_ids': torch.tensor(batch['input_ids']),\n",
    "                'attention_mask': torch.tensor(batch['attention_mask'])\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            pred_labels = decode_predictions(outputs.logits.cpu().numpy(), label_list)\n",
    "            # Reconstruct refs (skip -100 padding)\n",
    "            refs = [[label_list[l] if l != -100 else 'O' for l in labs] for labs in batch['labels']]\n",
    "            all_preds.extend(pred_labels)\n",
    "            all_refs.extend(refs)\n",
    "    \n",
    "    # Compute metrics\n",
    "    micro_f1 = f1_score(all_refs, all_preds)\n",
    "    micro_p = precision_score(all_refs, all_preds)\n",
    "    micro_r = recall_score(all_refs, all_preds)\n",
    "    report = classification_report(all_refs, all_preds, digits=3)\n",
    "    \n",
    "    return {\n",
    "        'f1': micro_f1,\n",
    "        'precision': micro_p,\n",
    "        'recall': micro_r,\n",
    "        'report': report\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Load trained model\n",
    "model_dir = MODELS_PREFERRED if MODELS_PREFERRED.exists() else MODELS_FALLBACK\n",
    "\n",
    "if not model_dir.exists():\n",
    "    print(f\"WARNING: No model found at {model_dir}\")\n",
    "    print(\"Train the model in Notebook 04 first.\")\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_dir)\n",
    "    print(f\"Loaded model from: {model_dir}\")\n",
    "    print(f\"  Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "    print(f\"  Model num labels: {model.config.num_labels}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Convert JSON data to BIO samples\n",
    "\n",
    "def to_bio_samples(split_data):\n",
    "    \"\"\"Convert character-span annotations to BIO token labels.\"\"\"\n",
    "    samples = []\n",
    "    for s in split_data:\n",
    "        text = s['text']\n",
    "        tokens = text.split()\n",
    "        tags = ['O'] * len(tokens)\n",
    "        \n",
    "        # Map token positions\n",
    "        spans = []\n",
    "        idx = 0\n",
    "        for tok in tokens:\n",
    "            start = text.find(tok, idx)\n",
    "            end = start + len(tok)\n",
    "            spans.append((start, end))\n",
    "            idx = end\n",
    "        \n",
    "        # Tag entities\n",
    "        for ent_start, ent_end, label in s.get('entities', []):\n",
    "            first = True\n",
    "            for ti, (ts, te) in enumerate(spans):\n",
    "                if ts < ent_end and te > ent_start:\n",
    "                    tags[ti] = ('B-' if first else 'I-') + label\n",
    "                    first = False\n",
    "        \n",
    "        samples.append({'tokens': tokens, 'labels': tags, 'text': text})\n",
    "    return samples\n",
    "\n",
    "\n",
    "if not (val_data or test_data):\n",
    "    print(\"No data available to evaluate.\")\n",
    "    val_ds = None\n",
    "    test_ds = None\n",
    "else:\n",
    "    val_ds = None\n",
    "    test_ds = None\n",
    "    \n",
    "    if val_data:\n",
    "        val_ds = Dataset.from_list(to_bio_samples(val_data))\n",
    "        print(f\"Created val dataset: {len(val_ds)} samples\")\n",
    "    \n",
    "    if test_data:\n",
    "        test_ds = Dataset.from_list(to_bio_samples(test_data))\n",
    "        print(f\"Created test dataset: {len(test_ds)} samples\")\n",
    "    \n",
    "    if val_ds or test_ds:\n",
    "        peek = (val_ds or test_ds)[0]\n",
    "        print(f\"Example tokens ({len(peek['tokens'])}): {peek['tokens'][:15]}...\")\n",
    "        print(f\"Example labels ({len(peek['labels'])}): {peek['labels'][:15]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Run evaluation and save metrics\n",
    "from datetime import datetime\n",
    "\n",
    "def save_metrics(metrics: Dict[str, Any], split_name: str):\n",
    "    \"\"\"Save metrics to results/model_metrics.\"\"\"\n",
    "    ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    out_path = RESULTS_DIR / f'ner_eval_{split_name}_{ts}.json'\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"  Saved to: {out_path}\")\n",
    "\n",
    "\n",
    "if model is None or not (val_ds or test_ds):\n",
    "    print(\"Skipping evaluation: missing model or data.\")\n",
    "else:\n",
    "    print(\"=\"*80)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    label_list = sorted(set(['O'] + [f'B-{l}' for l in labels] + [f'I-{l}' for l in labels]))\n",
    "    \n",
    "    if val_ds:\n",
    "        print(\"\\nValidation Set:\")\n",
    "        m_val = evaluate_model(model, tokenizer, val_ds, label_list)\n",
    "        print(m_val['report'])\n",
    "        print(f\"Micro F1: {m_val['f1']:.3f}  P: {m_val['precision']:.3f}  R: {m_val['recall']:.3f}\")\n",
    "        save_metrics(m_val, 'val')\n",
    "    \n",
    "    if test_ds:\n",
    "        print(\"\\nTest Set:\")\n",
    "        m_test = evaluate_model(model, tokenizer, test_ds, label_list)\n",
    "        print(m_test['report'])\n",
    "        print(f\"Micro F1: {m_test['f1']:.3f}  P: {m_test['precision']:.3f}  R: {m_test['recall']:.3f}\")\n",
    "        save_metrics(m_test, 'test')\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Qualitative examples\n",
    "\n",
    "if model is None or tokenizer is None:\n",
    "    print(\"Skipping examples: model/tokenizer not loaded.\")\n",
    "else:\n",
    "    def predict_entities(text: str):\n",
    "        \"\"\"Predict entities in text using the model.\"\"\"\n",
    "        toks = text.split()\n",
    "        enc = tokenizer(toks, is_split_into_words=True, return_tensors='pt', truncation=True)\n",
    "        import torch\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(**{k: v for k, v in enc.items() if k in ['input_ids', 'attention_mask']})\n",
    "        pred_ids = out.logits.argmax(-1).cpu().numpy()[0]\n",
    "        word_ids = enc.word_ids(0)\n",
    "        word_labels = []\n",
    "        used = set()\n",
    "        for wi, pid in zip(word_ids, pred_ids):\n",
    "            if wi is None or wi in used:\n",
    "                continue\n",
    "            used.add(wi)\n",
    "            word_labels.append(model.config.id2label.get(int(pid), 'O'))\n",
    "        return toks, word_labels\n",
    "    \n",
    "    dataset_for_examples = val_data or test_data or []\n",
    "    if not dataset_for_examples:\n",
    "        print(\"No examples available.\")\n",
    "    else:\n",
    "        print(\"=\"*80)\n",
    "        print(\"QUALITATIVE EXAMPLES (up to 3 random samples)\")\n",
    "        print(\"=\"*80)\n",
    "        for idx, ex in enumerate(random.sample(dataset_for_examples, min(3, len(dataset_for_examples))), 1):\n",
    "            toks, pred_tags = predict_entities(ex['text'])\n",
    "            gold_ents = ex.get('entities', [])\n",
    "            pred_ents = group_tokens_to_entities(toks, pred_tags)\n",
    "            text_preview = ex['text'][:150].replace('\\n', ' ') + ('...' if len(ex['text']) > 150 else '')\n",
    "            print(f\"\\nExample {idx}:\")\n",
    "            print(f\"  TEXT: {text_preview}\")\n",
    "            print(f\"  GOLD: {gold_ents}\")\n",
    "            print(f\"  PRED: {pred_ents}\")\n",
    "        print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Results have been saved to `results/model_metrics/ner_eval_*.json`.\n",
    "\n",
    "### Next Steps\n",
    "- Review metrics to assess model performance.\n",
    "- Compare gold vs. predicted entities in qualitative examples.\n",
    "- If F1 is low: expand annotations and retrain in Notebook 04.\n",
    "- If F1 is good: integrate the model into the detection pipeline."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOAOO6C8D31RJ7Xf0iUk5Tc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
