{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 07: App Interface Testing\n",
    "\n",
    "Validate the allergen detection pipeline through an app-like interface simulation. This notebook focuses on request/response shaping, latency checks, error handling, and UI-facing payloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Trained NER model available in `models/ner_model/`\n",
    "- OCR engine dependencies installed (`easyocr`, `opencv-python`)\n",
    "- Supporting files: `data/allergen_dictionary.json`, `data/ner_training/label_mapping.json`\n",
    "- Recommended: Run Notebook 06 once to verify pipeline\n",
    "\n",
    "Run cells in order. This notebook returns UI-ready JSON payloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A: Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setup paths and environment\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "if ROOT.name == \"notebooks\":\n",
    "    ROOT = ROOT.parent\n",
    "SRC = ROOT / \"src\"\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "MODELS_DIR = ROOT / \"models\"\n",
    "RESULTS_DIR = ROOT / \"results\"\n",
    "\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "\n",
    "print(\"✓ Paths configured\")\n",
    "print(f\"Root: {ROOT}\")\n",
    "print(f\"Data: {DATA_DIR}\")\n",
    "print(f\"Models: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Imports\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from ocr.simple_ocr_engine import SimpleOCREngine\n",
    "\n",
    "print(\"✓ Imports loaded\")\n",
    "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B: Load Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load allergen dictionary and label mapping\n",
    "allergen_path = DATA_DIR / \"allergen_dictionary.json\"\n",
    "label_mapping_path = DATA_DIR / \"ner_training\" / \"label_mapping.json\"\n",
    "\n",
    "with open(allergen_path, 'r') as f:\n",
    "    allergen_dictionary = json.load(f)\n",
    "with open(label_mapping_path, 'r') as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "id2label = {int(k): v for k, v in label_mapping[\"id2label\"].items()}\n",
    "label2id = {v: int(k) for k, v in label_mapping[\"label2id\"].items()}\n",
    "\n",
    "print(f\"✓ Allergen dictionary loaded: {len(allergen_dictionary)} types\")\n",
    "print(f\"✓ Labels loaded: {list(id2label.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load trained NER model and tokenizer\n",
    "model_path = MODELS_DIR / \"ner_model\"\n",
    "if not model_path.exists():\n",
    "    experiments = list((MODELS_DIR / \"experiments\").glob(\"**/pytorch_model.bin\"))\n",
    "    if experiments:\n",
    "        model_path = experiments[0].parent\n",
    "        print(f\"ℹ️  Using model from experiments: {model_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No trained model found. Run Notebook 04 first.\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device).eval()\n",
    "    print(f\"✓ Model loaded on device: {device}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Initialize OCR engine\n",
    "try:\n",
    "    ocr_engine = SimpleOCREngine(lang_list=[\"en\"], gpu=torch.cuda.is_available())\n",
    "    print(\"✓ OCR engine initialized\")\n",
    "except Exception as e:\n",
    "    ocr_engine = None\n",
    "    print(f\"⚠️  OCR unavailable: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section C: API-Style Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Helper functions (text cleaning, NER, mapping, response formatting)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.replace('_', '')\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def run_ner_prediction(text: str) -> List[Tuple[str, str, float]]:\n",
    "    if not text or len(text.strip()) < 3:\n",
    "        return []\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=True,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    offsets = inputs.pop(\"offset_mapping\")[0].numpy()\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        confs = probs.max(dim=-1).values.cpu().numpy()\n",
    "    \n",
    "    entities = []\n",
    "    current = None\n",
    "    conf_list = []\n",
    "    start_idx = 0\n",
    "    for pred, conf, (s, e) in zip(preds, confs, offsets):\n",
    "        if s == e:\n",
    "            continue\n",
    "        label = id2label[pred]\n",
    "        if label.startswith('B-'):\n",
    "            if current:\n",
    "                entities.append((current, curr_label, float(np.mean(conf_list))))\n",
    "            current = text[s:e]\n",
    "            curr_label = label[2:]\n",
    "            conf_list = [conf]\n",
    "            start_idx = s\n",
    "        elif label.startswith('I-') and current:\n",
    "            current = text[start_idx:e]\n",
    "            conf_list.append(conf)\n",
    "        else:\n",
    "            if current:\n",
    "                entities.append((current, curr_label, float(np.mean(conf_list))))\n",
    "                current = None\n",
    "                conf_list = []\n",
    "    if current:\n",
    "        entities.append((current, curr_label, float(np.mean(conf_list))))\n",
    "    return entities\n",
    "\n",
    "\n",
    "def map_to_standard_allergens(entities: List[Tuple[str, str, float]]) -> Dict:\n",
    "    detected = defaultdict(list)\n",
    "    for text_span, label, conf in entities:\n",
    "        span_lower = text_span.lower().strip()\n",
    "        matched = False\n",
    "        for allergen_type, synonyms in allergen_dictionary.items():\n",
    "            for syn in synonyms:\n",
    "                if syn.lower() in span_lower or span_lower in syn.lower():\n",
    "                    detected[allergen_type].append({\n",
    "                        'text': text_span,\n",
    "                        'label': label,\n",
    "                        'confidence': conf\n",
    "                    })\n",
    "                    matched = True\n",
    "                    break\n",
    "            if matched:\n",
    "                break\n",
    "        if not matched and label != 'O':\n",
    "            detected['unknown'].append({\n",
    "                'text': text_span,\n",
    "                'label': label,\n",
    "                'confidence': conf\n",
    "            })\n",
    "    return dict(detected)\n",
    "\n",
    "\n",
    "def format_response(result: Dict) -> Dict:\n",
    "    return {\n",
    "        'image_path': result.get('image_path'),\n",
    "        'success': result.get('success', False),\n",
    "        'error': result.get('error'),\n",
    "        'raw_text': result.get('raw_text', ''),\n",
    "        'cleaned_text': result.get('cleaned_text', ''),\n",
    "        'detected_allergens': result.get('detected_allergens', {}),\n",
    "        'avg_confidence': result.get('avg_confidence', 0.0),\n",
    "        'timings': result.get('timings', {}),\n",
    "        'entities_found': result.get('entities_found', [])\n",
    "    }\n",
    "\n",
    "print(\"✓ Helper functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: API-style detection function\n",
    "\n",
    "def detect_allergens_api(request: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Simulate an app/API call.\n",
    "    Expected request fields:\n",
    "      - image_path: path to image (str)\n",
    "      - use_ocr: bool (default True)\n",
    "      - provided_text: optional str (used if use_ocr=False or as fallback)\n",
    "    Returns UI-ready JSON payload.\n",
    "    \"\"\"\n",
    "    image_path = request.get('image_path')\n",
    "    use_ocr = request.get('use_ocr', True)\n",
    "    provided_text = request.get('provided_text')\n",
    "    verbose = request.get('verbose', False)\n",
    "    \n",
    "    resp = {\n",
    "        'image_path': image_path,\n",
    "        'success': False,\n",
    "        'error': None,\n",
    "        'raw_text': '',\n",
    "        'cleaned_text': '',\n",
    "        'detected_allergens': {},\n",
    "        'avg_confidence': 0.0,\n",
    "        'entities_found': [],\n",
    "        'timings': {},\n",
    "    }\n",
    "    try:\n",
    "        if not image_path:\n",
    "            raise ValueError('image_path is required')\n",
    "        \n",
    "        # Step A: OCR or provided text\n",
    "        t0 = time.time()\n",
    "        if use_ocr:\n",
    "            if ocr_engine is None:\n",
    "                raise RuntimeError('OCR engine not available')\n",
    "            img = cv2.imread(str(image_path))\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Cannot read image: {image_path}\")\n",
    "            raw_text = ocr_engine.extract(img)\n",
    "        else:\n",
    "            if not provided_text:\n",
    "                raise ValueError('provided_text is required when use_ocr=False')\n",
    "            raw_text = provided_text\n",
    "        resp['timings']['ocr'] = time.time() - t0\n",
    "        resp['raw_text'] = raw_text\n",
    "        \n",
    "        # Step B: Clean text\n",
    "        t0 = time.time()\n",
    "        cleaned = clean_text(raw_text)\n",
    "        resp['cleaned_text'] = cleaned\n",
    "        resp['timings']['cleaning'] = time.time() - t0\n",
    "        if not cleaned:\n",
    "            raise ValueError('No text after cleaning')\n",
    "        \n",
    "        # Step C: NER\n",
    "        t0 = time.time()\n",
    "        entities = run_ner_prediction(cleaned)\n",
    "        resp['entities_found'] = entities\n",
    "        resp['timings']['ner'] = time.time() - t0\n",
    "        \n",
    "        # Step D: Mapping\n",
    "        t0 = time.time()\n",
    "        mapped = map_to_standard_allergens(entities)\n",
    "        resp['detected_allergens'] = mapped\n",
    "        resp['timings']['mapping'] = time.time() - t0\n",
    "        \n",
    "        # Step E: Confidence\n",
    "        confs = [d['confidence'] for v in mapped.values() for d in v]\n",
    "        resp['avg_confidence'] = float(np.mean(confs)) if confs else 0.0\n",
    "        resp['success'] = True\n",
    "        resp['timings']['total'] = sum(resp['timings'].values())\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ {image_path} | allergens: {list(mapped.keys())} | conf: {resp['avg_confidence']:.2f}\")\n",
    "    except Exception as e:\n",
    "        resp['error'] = str(e)\n",
    "        if verbose:\n",
    "            print(f\"❌ {image_path}: {e}\")\n",
    "    return resp\n",
    "\n",
    "print(\"✓ API-style detection function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section D: UI Scenario Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Load test images and optional ground-truth texts\n",
    "\n",
    "test_samples_dir = DATA_DIR / \"ocr_results\" / \"test_samples\"\n",
    "raw_dir = DATA_DIR / \"raw\"\n",
    "\n",
    "test_images = []\n",
    "if test_samples_dir.exists():\n",
    "    test_images.extend(list(test_samples_dir.glob('*.jpg')))\n",
    "    test_images.extend(list(test_samples_dir.glob('*.png')))\n",
    "if len(test_images) < 5 and raw_dir.exists():\n",
    "    test_images.extend(list(raw_dir.glob('*.jpg'))[:10])\n",
    "\n",
    "# deduplicate\n",
    "test_images = list({p.name: p for p in test_images}.values())\n",
    "\n",
    "print(f\"Found {len(test_images)} test images\")\n",
    "for p in test_images[:5]:\n",
    "    print(f\" - {p.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Simulate API calls for UI responses\n",
    "\n",
    "ui_responses = []\n",
    "max_samples = min(10, len(test_images))\n",
    "\n",
    "for img_path in test_images[:max_samples]:\n",
    "    txt_path = img_path.with_suffix('.txt')\n",
    "    provided_text = None\n",
    "    if txt_path.exists():\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            provided_text = f.read()\n",
    "    \n",
    "    request = {\n",
    "        'image_path': str(img_path),\n",
    "        'use_ocr': ocr_engine is not None,\n",
    "        'provided_text': provided_text,\n",
    "        'verbose': True\n",
    "    }\n",
    "    resp = detect_allergens_api(request)\n",
    "    ui_responses.append(resp)\n",
    "\n",
    "print(f\"\\n✓ Completed {len(ui_responses)} simulated calls\")\n",
    "success_count = sum(1 for r in ui_responses if r['success'])\n",
    "print(f\"Success rate: {success_count}/{len(ui_responses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section E: Response Validation and UX Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Validate response schema and preview\n",
    "\n",
    "required_keys = {\"image_path\", \"success\", \"error\", \"detected_allergens\", \"avg_confidence\", \"timings\"}\n",
    "\n",
    "schema_issues = []\n",
    "for resp in ui_responses:\n",
    "    missing = required_keys - set(resp.keys())\n",
    "    if missing:\n",
    "        schema_issues.append((resp.get('image_path'), list(missing)))\n",
    "\n",
    "if schema_issues:\n",
    "    print(\"⚠️ Schema issues found:\")\n",
    "    for img, miss in schema_issues:\n",
    "        print(f\" - {img}: missing {miss}\")\n",
    "else:\n",
    "    print(\"✓ All responses conform to schema\")\n",
    "\n",
    "# Preview first 3 responses\n",
    "for resp in ui_responses[:3]:\n",
    "    print(\"\\n--- Response Preview ---\")\n",
    "    print(f\"Image: {resp['image_path']}\")\n",
    "    print(f\"Success: {resp['success']} | Allergens: {list(resp['detected_allergens'].keys())}\")\n",
    "    print(f\"Avg confidence: {resp['avg_confidence']:.2f}\")\n",
    "    print(f\"Error: {resp['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section F: Latency and Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Compute latency stats\n",
    "\n",
    "components = ['ocr', 'cleaning', 'ner', 'mapping', 'total']\n",
    "latency = {c: [] for c in components}\n",
    "\n",
    "for resp in ui_responses:\n",
    "    if resp.get('timings'):\n",
    "        for c in components:\n",
    "            if c == 'total':\n",
    "                latency[c].append(resp['timings'].get('total', sum(resp['timings'].values())))\n",
    "            else:\n",
    "                latency[c].append(resp['timings'].get(c, 0))\n",
    "\n",
    "print(\"Latency summary (seconds):\")\n",
    "for c in components:\n",
    "    if latency[c]:\n",
    "        arr = np.array(latency[c])\n",
    "        print(f\"  {c:<8} | avg: {arr.mean():.3f} | p95: {np.percentile(arr,95):.3f} | max: {arr.max():.3f}\")\n",
    "\n",
    "if latency['total']:\n",
    "    avg_total = np.mean(latency['total'])\n",
    "    throughput = 1.0 / avg_total if avg_total > 0 else 0\n",
    "    print(f\"\\nThroughput: {throughput:.2f} images/sec ({throughput*60:.1f} per minute)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section G: Save Responses (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Save UI responses to JSON\n",
    "from datetime import datetime\n",
    "output_dir = RESULTS_DIR / \"ui_testing\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "ui_path = output_dir / f\"ui_responses_{stamp}.json\"\n",
    "\n",
    "with open(ui_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(ui_responses, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✓ Saved {len(ui_responses)} responses to {ui_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "- Validated API-style responses for allergen detection\n",
    "- Collected latency stats for UI performance budgets\n",
    "- Saved sample payloads for frontend integration testing\n",
    "\n",
    "**Next:**\n",
    "1) Integrate with frontend / backend service\n",
    "2) Add retries or fallbacks for OCR failures\n",
    "3) Tune confidence thresholds based on UI feedback\n",
    "4) Expand test set and run batch mode if needed"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOAOO6C8D31RJ7Xf0iUk5Tc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
